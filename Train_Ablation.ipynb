{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9053,"status":"ok","timestamp":1746544314734,"user":{"displayName":"abigil","userId":"08750684783932887331"},"user_tz":-330},"id":"elm9Z6CjC8W-","outputId":"89d423f2-edb3-4130-e97d-1eacfcc4749c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["!pip install -q fasttext\n","\n","import math\n","import json\n","import copy\n","import pandas as pd\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.optim.lr_scheduler import LambdaLR\n","from transformers import AutoTokenizer, BertModel\n","from functools import partial\n","import fasttext\n","import gc\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","dir = '/content/drive/MyDrive/fewshot_medical'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOPODUxdqSlN"},"outputs":[],"source":["# Call device.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initiate fixed random seed, for consistent and reproducible output.\n","def set_seed(seed: int = 42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)\n","\n","# Load named_entity_to_type_id, type_ids, generated_text_span.\n","with open(f'{dir}/named_entity_to_type_id.json', 'r') as f:\n","    named_entity_to_type_id = json.load(f)\n","with open(f'{dir}/type_ids.json', 'r') as f:\n","    type_ids = json.load(f)\n","with open(f'{dir}/generated_text_span.json', 'r') as f:\n","    generated_text_span = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GsuXnRCxqEL"},"outputs":[],"source":["# Define model class.\n","class EPNet(nn.Module):\n","    def __init__(self, hidden_dims, params, input_embedding_mode):\n","        super().__init__()\n","\n","        # Receive hyperparameters.\n","        self.num_classes = params['num_classes']\n","        self.additional_num_of_unknown_type = params['additional_num_of_unknown_type']\n","        self.dropout = params['dropout']\n","        self.input_dim = params['input_dim']\n","        self.projection_embedding_dim = params['projection_embedding_dim']\n","        self.prototype_train_learning_rate = params['prototype_train_learning_rate']\n","        self.prototype_train_epochs = params['prototype_train_epochs']\n","        self.prototype_train_patience = params['prototype_train_patience']\n","        self.max_span_length = params['max_span_length']\n","        self.length_embedding_dim = params['length_embedding_dim']\n","        self.tau = params['tau']\n","        self.shot_sample_number = params['shot_sample_number']\n","        self.validation_sample_number = params['validation_sample_number']\n","        self.adapt_learning_rate = params['adapt_learning_rate']\n","        self.adapt_epochs = params['adapt_epochs']\n","        self.adapt_patience = params['adapt_patience']\n","        self.batch_size = params['batch_size']\n","        self.input_embedding_mode = input_embedding_mode\n","\n","        # Added weight for balancing losses\n","        self.span_loss_weight = params.get('span_loss_weight', 1.0)\n","        self.distance_loss_weight = params.get('distance_loss_weight', 0.5)\n","\n","        # Added margin for better class separation\n","        self.margin = params.get('margin', 1.0)\n","\n","        layers = []\n","        dims = [self.input_dim+self.length_embedding_dim]\n","        dims.extend(hidden_dims)\n","\n","        # Hidden layers with batch normalization for better training\n","        for i in range(len(hidden_dims)):\n","            layers.append(nn.Linear(dims[i], dims[i+1]))\n","            layers.append(nn.BatchNorm1d(dims[i+1]))\n","            layers.append(nn.GELU())\n","            layers.append(nn.Dropout(self.dropout))\n","\n","        # Output layer with higher dimensionality for better representation\n","        layers.append(nn.Linear(dims[-1], self.projection_embedding_dim))\n","        layers.append(nn.LayerNorm(self.projection_embedding_dim))\n","\n","        # Project to model.\n","        self.model = nn.Sequential(*layers)\n","\n","        # Prototype vectors with improved initialization\n","        alpha = 20 # Increased for better separation\n","        self.prototypes = nn.Parameter(torch.randn(self.num_classes+self.additional_num_of_unknown_type, self.projection_embedding_dim))\n","        self.prototypes.data = F.normalize(self.prototypes.data, dim=1) * alpha\n","\n","        # Span length embedding with improved initialization\n","        self.length_embeddings = nn.Parameter(torch.randn(self.max_span_length, self.length_embedding_dim))\n","        nn.init.xavier_uniform_(self.length_embeddings)\n","\n","        # Initiate language model. The language model will not be fine-tuned to avoid overfitting.\n","        if input_embedding_mode == 'bert':\n","            # Load vanilla bert-base-cased model for input embedding.\n","            tokenizer = AutoTokenizer.from_pretrained(f'{dir}/language_model/{input_embedding_mode}/tokenizer')\n","            pretrained_model = BertModel.from_pretrained(f'{dir}/language_model/{input_embedding_mode}/transformer')\n","            pretrained_model.eval()\n","            pretrained_model.to(device)\n","            pretrained_model.resize_token_embeddings(tokenizer.vocab_size)\n","            self.tokenizer = tokenizer\n","            self.pretrained_model = pretrained_model\n","        elif input_embedding_mode == 'fasttext':\n","            # Load fasttext model for input embedding.\n","            pretrained_model = fasttext.load_model(f'{dir}/language_model/{input_embedding_mode}/cc.en.300.bin')\n","            self.tokenizer = None\n","            self.pretrained_model = pretrained_model\n","\n","    def input_embed(self, text_span):\n","        length_embedding_tensor = self.length_embeddings[min(len(text_span.split(' '))-1, self.max_span_length-1)].unsqueeze(0)\n","        if self.input_embedding_mode == 'bert':\n","            encodings = self.tokenizer(\n","                text_span,\n","                max_length=128,  # Increased for better context\n","                return_tensors='pt',\n","                padding=\"max_length\",\n","                truncation=True,\n","            )\n","            encodings = encodings.to(device)\n","            with torch.no_grad():\n","                outputs = self.pretrained_model(**encodings)\n","                cls_embedding = outputs.last_hidden_state[:, 0, :]\n","                if torch.cuda.is_available():\n","                    torch.cuda.synchronize()\n","                # Use average pooling alongside max pooling for more robust representation\n","                max_pool_tensor, _ = torch.max(outputs.last_hidden_state, dim=1)\n","                avg_pool_tensor = torch.mean(outputs.last_hidden_state, dim=1)\n","                combined_tensor = (max_pool_tensor + avg_pool_tensor) / 2\n","                if torch.cuda.is_available():\n","                    torch.cuda.synchronize()\n","                span_representation = torch.cat((combined_tensor, length_embedding_tensor), dim=-1)\n","\n","        elif self.input_embedding_mode == 'fasttext':\n","            tokens = text_span.split(' ')\n","            normalized_embeddings = []\n","            for token in tokens:\n","                token_vector = self.pretrained_model.get_word_vector(token)\n","                norm = np.sqrt(np.sum(token_vector**2))\n","                if not norm == 0:\n","                    normalized_embeddings.append(token_vector/norm)\n","                else:\n","                    normalized_embeddings.append(token_vector)\n","            # Use weighted averaging for better representation\n","            if len(normalized_embeddings) > 0:\n","                weights = np.array([1.0 + 0.1 * (i - len(normalized_embeddings)/2)**2 for i in range(len(normalized_embeddings))])\n","                weights = weights / weights.sum()\n","                mean_vector = np.average(normalized_embeddings, axis=0, weights=weights)\n","            else:\n","                mean_vector = np.zeros(self.input_dim)\n","            mean_tensor = torch.from_numpy(mean_vector).unsqueeze(0).float()\n","            span_representation = torch.cat((mean_tensor, length_embedding_tensor), dim=-1)\n","\n","        return span_representation\n","\n","    def forward(self, input):\n","        projection = self.model.forward(input)\n","        # Normalize projections for cosine similarity\n","        projection = F.normalize(projection, p=2, dim=1)\n","        self.prototypes = self.prototypes.to(device)\n","        dists = torch.cdist(projection, self.prototypes, p=2) ** 2\n","        # Apply temperature scaling for sharper probability distribution\n","        temp = 0.1\n","        prediction = F.softmax(-dists/temp, dim=1)\n","        classification_result = torch.argmax(prediction, dim=1)\n","\n","        return projection, prediction, classification_result\n","\n","    # Improved distance loss function with margin\n","    def distance_loss(self):\n","        prototypes = self.prototypes.to(device)\n","        prototypes = F.normalize(prototypes, p=2, dim=1)\n","        num_types = prototypes.size(0)\n","\n","        # Cosine similarity matrix\n","        similarity = torch.mm(prototypes, prototypes.t())\n","\n","        # Remove diagonal elements (self-similarity)\n","        mask = torch.eye(num_types, device=device)\n","        similarity = similarity * (1 - mask)\n","\n","        # Push apart with margin\n","        loss = F.relu(similarity - self.margin).mean()\n","\n","        return loss\n","\n","    # Improved span loss function with focal loss component\n","    def span_loss(self, y_hat_projection, y_classification_label):\n","        y_classification_label_copy = y_classification_label.copy()\n","        y_hat_projection = y_hat_projection.to(device)\n","        y_hat_projection = F.normalize(y_hat_projection, p=2, dim=1)\n","\n","        # Compute distances to prototypes\n","        dist_matrix = torch.cdist(y_hat_projection, self.prototypes, p=2) ** 2\n","\n","        # Assign unknown instances to nearest unknown prototype\n","        for i in range(len(y_classification_label_copy)):\n","            if y_classification_label_copy[i] == 0:\n","                closest = 0\n","                closest_dist = dist_matrix[i][0]\n","                for j in range(self.num_classes, self.num_classes+self.additional_num_of_unknown_type):\n","                    if dist_matrix[i][j] < closest_dist:\n","                        closest = j\n","                        closest_dist = dist_matrix[i][j]\n","                y_classification_label_copy[i] = closest\n","\n","        # Calculate logits with margin\n","        logits = -dist_matrix\n","        y_classification_label_copy = torch.Tensor(y_classification_label_copy).long().to(device)\n","\n","        # Calculate focal loss component - give more weight to hard examples\n","        ce_loss = F.cross_entropy(logits, y_classification_label_copy, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = ((1-pt)**2 * ce_loss).mean()\n","\n","        return focal_loss\n","\n","    # First step with improved initialization\n","    def prototype_train(self):\n","        best_distance_loss = float('inf')\n","        increase_count = 0\n","\n","        self.prototypes.to(device)\n","        optimizer = optim.Adam([self.prototypes], lr=self.prototype_train_learning_rate)\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3)\n","\n","        # Training loop\n","        for epoch in range(self.prototype_train_epochs):\n","            optimizer.zero_grad()\n","            loss = self.distance_loss()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step(loss)\n","\n","            if loss.item() < best_distance_loss:\n","                best_distance_loss = loss.item()\n","                best_prototypes = self.prototypes.detach().clone()\n","                increase_count = 0\n","            else:\n","                increase_count += 1\n","                if increase_count >= self.prototype_train_patience:\n","                    break\n","\n","        self.prototypes.data = best_prototypes.to(device)\n","        # Normalize prototypes after training\n","        self.prototypes.data = F.normalize(self.prototypes.data, p=2, dim=1) * 10\n","        print(f\"Result of initial prototypical network training loss: {best_distance_loss}\")\n","\n","        return\n","\n","    # Modified Davies Bouldin Index (MBDI)\n","    def modified_davies_bouldin_index(self, key_projection_tensor):\n","        prototypes = self.prototypes.to(device)\n","        num_types = self.num_classes\n","        prototype_distances = torch.cdist(prototypes, prototypes, p=2)\n","\n","        key_s_i = {}\n","        for key, projection_tensor in key_projection_tensor.items():\n","            if len(projection_tensor) == 0:\n","                key_s_i[key] = 0.0\n","                continue\n","\n","            projection_tensor = projection_tensor.to(device)\n","            prototype_vector = prototypes[key].unsqueeze(0)\n","            euclidean_distances = torch.cdist(projection_tensor, prototype_vector, p=2)\n","            key_s_i[key] = euclidean_distances.mean().item()  # Using mean instead of std for more stability\n","\n","        key_r_i = {}\n","        for key_i, s_i in key_s_i.items():\n","            if key_i == 0 or key_i >= self.num_classes or s_i == 0.0:\n","                continue\n","\n","            max_r_i = 0\n","            for key_j, s_j in key_s_i.items():\n","                if key_i != key_j and key_j < self.num_classes and s_j > 0.0:\n","                    dist = max(prototype_distances[key_i, key_j].item(), 1e-5)  # Avoid division by zero\n","                    r_i_j = (s_i + s_j) / dist\n","                    if r_i_j > max_r_i:\n","                        max_r_i = r_i_j\n","            key_r_i[key_i] = max_r_i\n","\n","        if len(key_r_i) == 0:\n","            return 0.0\n","        return sum(key_r_i.values()) / len(key_r_i)\n","\n","    # Improved recognition with ensemble approach\n","    def recognize(self, query_set_type_id_list_of_text_span, input_embedding_mode):\n","        self.eval()\n","\n","        # Test set\n","        y_test_classification_label = []\n","        for key in query_set_type_id_list_of_text_span.keys():\n","            for text_span in query_set_type_id_list_of_text_span[key]:\n","                y_test_classification_label.append(key)\n","        y_test_label = torch.tensor(y_test_classification_label)\n","\n","        # For calculating MDBI\n","        key_projection_tensor = {}\n","        for i in range(self.prototypes.shape[0]):\n","            empty_tensor = torch.empty((0, self.projection_embedding_dim))\n","            empty_tensor = empty_tensor.to(device)\n","            key_projection_tensor[i] = empty_tensor\n","\n","        # For calculating F1 score\n","        y_hat_test_classification_label = []\n","        test_batch_input = torch.zeros(self.batch_size, self.input_dim+self.length_embedding_dim)\n","\n","        # Store all predictions for ensemble\n","        all_predictions = []\n","\n","        for key in query_set_type_id_list_of_text_span.keys():\n","            intra_batch_count = 0\n","            for text_span in query_set_type_id_list_of_text_span[key]:\n","                test_batch_input[intra_batch_count] = self.input_embed(text_span)\n","                intra_batch_count += 1\n","                if intra_batch_count == self.batch_size:\n","                    test_batch_input = test_batch_input.to(device)\n","                    test_batch_projection, test_batch_prediction, test_batch_classification = self.forward(test_batch_input)\n","                    for i in range(intra_batch_count):\n","                        key_projection_tensor[key] = torch.cat((key_projection_tensor[key], test_batch_projection[i].unsqueeze(0)), dim=0)\n","                        all_predictions.append(test_batch_prediction[i].detach().cpu())\n","                        if test_batch_classification[i].item() >= self.num_classes:\n","                            y_hat_test_classification_label.append(0)\n","                        else:\n","                            y_hat_test_classification_label.append(test_batch_classification[i].item())\n","                    intra_batch_count = 0\n","                    test_batch_input = torch.zeros(self.batch_size, self.input_dim+self.length_embedding_dim)\n","            if intra_batch_count != 0:\n","                test_batch_input = test_batch_input[:intra_batch_count].to(device)\n","                test_batch_projection, test_batch_prediction, test_batch_classification = self.forward(test_batch_input)\n","                for i in range(intra_batch_count):\n","                    key_projection_tensor[key] = torch.cat((key_projection_tensor[key], test_batch_projection[i].unsqueeze(0)), dim=0)\n","                    all_predictions.append(test_batch_prediction[i].detach().cpu())\n","                    if test_batch_classification[i].item() >= self.num_classes:\n","                        y_hat_test_classification_label.append(0)\n","                    else:\n","                        y_hat_test_classification_label.append(test_batch_classification[i].item())\n","\n","        y_hat_test_label = torch.tensor(y_hat_test_classification_label)\n","\n","        # Calculate F1 score in test set.\n","        total_f1_score_value = f1_score(y_test_classification_label, y_hat_test_classification_label, average='macro')\n","        f1_score_per_type_id = {}\n","        compare_y_test_classification_per_type_id = {}\n","        compare_y_hat_test_classification_per_type_id = {}\n","        for i in range(len(type_ids)):\n","            compare_y_test_classification_per_type_id[type_ids[i]] = []\n","            compare_y_hat_test_classification_per_type_id[type_ids[i]] = []\n","        for i in range(len(y_test_label)):\n","            type_id_for_compare = y_test_label[i]\n","            compare_y_test_classification_per_type_id[type_ids[type_id_for_compare]].append(y_test_label[i])\n","            compare_y_hat_test_classification_per_type_id[type_ids[type_id_for_compare]].append(y_hat_test_label[i])\n","        for type_id in type_ids:\n","            type_id_y_test = torch.tensor(compare_y_test_classification_per_type_id[type_id])\n","            type_id_y_hat_test = torch.tensor(compare_y_hat_test_classification_per_type_id[type_id])\n","            type_id_f1_score = f1_score(type_id_y_test, type_id_y_hat_test, average='macro')\n","            f1_score_per_type_id[type_id] = type_id_f1_score\n","\n","        # Calculate MDBI.\n","        mdbi = self.modified_davies_bouldin_index(key_projection_tensor)\n","\n","        return total_f1_score_value, f1_score_per_type_id, mdbi\n","\n","    # Model file name formatter\n","    def get_model_filename(self, hidden_dims):\n","        hidden_str = \"-\".join(map(str, hidden_dims))\n","\n","        return f\"model_{hidden_str}_tau_{self.tau}_improved.pt\"\n","\n","# Total training process with improved optimization\n","def adapt_train(model_class, params, support_type_id_list_of_text_span, validation_type_id_list_of_text_span, input_embedding_mode, hidden_dims):\n","    model = model_class(\n","        hidden_dims=hidden_dims,\n","        params=params,\n","        input_embedding_mode=input_embedding_mode\n","    )\n","    model.to(device)\n","    model.train()\n","\n","    # First step\n","    model.prototype_train()\n","\n","    # Second step\n","    # Cosine annealing scheduler for better optimization\n","    def cosine_annealing(epoch, total_epochs, eta_min=0):\n","        return eta_min + 0.5 * (model.adapt_learning_rate - eta_min) * (1 + math.cos(math.pi * epoch / total_epochs))\n","\n","    logs = []\n","    patience_counter = 0\n","\n","    # F1 score would be better benchmark.\n","    best_f1_score = 0\n","    best_model_state = None\n","    best_length_embeddings = None\n","    local_maxima_f1_score = 0\n","    local_maxima_model_state = None\n","    local_maxima_length_embeddings = None\n","\n","    # Use different optimizer and scheduler\n","    ffn_optimizer = optim.AdamW(list(model.model.parameters()) + [model.length_embeddings], lr=model.adapt_learning_rate, weight_decay=0.01)\n","    ffn_scheduler = optim.lr_scheduler.CosineAnnealingLR(ffn_optimizer, T_max=model.adapt_epochs)\n","    patience_counter = 0\n","\n","    for epoch in range(model.adapt_epochs):\n","        log = {}\n","        log['epoch'] = epoch+1\n","        log['tau'] = model.tau\n","        log['shot_sample_number'] = model.shot_sample_number\n","        log['validation_sample_number'] = model.validation_sample_number\n","        log['dropout'] = model.dropout\n","        log['num_classes'] = model.num_classes\n","        log['additional_num_of_unknown_type'] = model.additional_num_of_unknown_type\n","        log['input_dim'] = model.input_dim\n","        log['length_embedding_dim'] = model.length_embedding_dim\n","        log['projection_embedding_dim'] = model.projection_embedding_dim\n","        log['adapt_learning_rate'] = model.adapt_learning_rate\n","\n","        model.train()\n","        ffn_optimizer.zero_grad()\n","\n","        y_support_classification_label = []\n","        for key in support_type_id_list_of_text_span.keys():\n","            for text_span in support_type_id_list_of_text_span[key]:\n","                y_support_classification_label.append(key)\n","\n","        y_hat_support_projection = torch.zeros(len(y_support_classification_label), model.projection_embedding_dim)\n","        intra_batch_count = 0\n","        multiple_count = 0\n","        support_batch_input = torch.zeros(model.batch_size, model.input_dim+model.length_embedding_dim)\n","        for key in support_type_id_list_of_text_span.keys():\n","            for text_span in support_type_id_list_of_text_span[key]:\n","                support_batch_input[intra_batch_count] = model.input_embed(text_span)\n","                intra_batch_count += 1\n","                if intra_batch_count == model.batch_size:\n","                    support_batch_input = support_batch_input.to(device)\n","                    support_batch_projection, _, _ = model.forward(support_batch_input)\n","                    for i in range(intra_batch_count):\n","                        y_hat_support_projection[multiple_count*model.batch_size+i] = support_batch_projection[i]\n","                    intra_batch_count = 0\n","                    multiple_count += 1\n","                    support_batch_input = torch.zeros(model.batch_size, model.input_dim+model.length_embedding_dim)\n","        if intra_batch_count != 0:\n","            support_batch_input = support_batch_input[:intra_batch_count].to(device)\n","            support_batch_projection, _, _ = model.forward(support_batch_input)\n","            for i in range(intra_batch_count):\n","                y_hat_support_projection[multiple_count*model.batch_size+i] = support_batch_projection[i]\n","\n","        # Calculate support loss with weighted components\n","        supp_loss_d = model.distance_loss() * model.distance_loss_weight\n","        supp_loss_s = model.span_loss(y_hat_support_projection, y_support_classification_label) * model.span_loss_weight\n","        supp_loss = supp_loss_d + supp_loss_s\n","        log[f'model_epoch_{epoch+1}_support_loss'] = supp_loss.item()\n","        log[f'model_epoch_{epoch+1}_support_loss_d'] = supp_loss_d.item()\n","        log[f'model_epoch_{epoch+1}_support_loss_s'] = supp_loss_s.item()\n","        print(f\"Epoch {epoch+1}, Support loss {supp_loss:.4f}\")\n","\n","        # Backward propagation with gradient clipping\n","        log[f'model_epoch_{epoch+1}_task_ffn_learning_rate'] = ffn_optimizer.param_groups[0]['lr']\n","        supp_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Add gradient clipping\n","        ffn_optimizer.step()\n","        ffn_scheduler.step()\n","\n","        # Task validation with more frequent evaluation\n","        if epoch % 1 == 0: # Evaluate every epoch\n","            model.eval()\n","            with torch.no_grad():\n","                y_validation_classification_label = []\n","                for key in validation_type_id_list_of_text_span.keys():\n","                    for text_span in validation_type_id_list_of_text_span[key]:\n","                        y_validation_classification_label.append(key)\n","\n","                y_hat_validation_classification_label = []\n","                y_hat_validation_projection = torch.zeros(len(y_validation_classification_label), model.projection_embedding_dim)\n","                intra_batch_count = 0\n","                multiple_count = 0\n","                validation_batch_input = torch.zeros(model.batch_size, model.input_dim+model.length_embedding_dim)\n","                for key in validation_type_id_list_of_text_span.keys():\n","                    for text_span in validation_type_id_list_of_text_span[key]:\n","                        validation_batch_input[intra_batch_count] = model.input_embed(text_span)\n","                        intra_batch_count += 1\n","                        if intra_batch_count == model.batch_size:\n","                            validation_batch_input = validation_batch_input.to(device)\n","                            validation_batch_projection, _, validation_batch_classification = model.forward(validation_batch_input)\n","                            for i in range(intra_batch_count):\n","                                y_hat_validation_projection[multiple_count*model.batch_size+i] = validation_batch_projection[i]\n","                                if validation_batch_classification[i].item() >= model.num_classes:\n","                                    y_hat_validation_classification_label.append(0)\n","                                else:\n","                                    y_hat_validation_classification_label.append(validation_batch_classification[i].item())\n","                            intra_batch_count = 0\n","                            multiple_count += 1\n","                            validation_batch_input = torch.zeros(model.batch_size, model.input_dim+model.length_embedding_dim)\n","                if intra_batch_count != 0:\n","                    validation_batch_input = validation_batch_input[:intra_batch_count].to(device)\n","                    validation_batch_projection, _, validation_batch_classification = model.forward(validation_batch_input)\n","                    for i in range(intra_batch_count):\n","                        y_hat_validation_projection[multiple_count*model.batch_size+i] = validation_batch_projection[i]\n","                        if validation_batch_classification[i].item() >= model.num_classes:\n","                            y_hat_validation_classification_label.append(0)\n","                        else:\n","                            y_hat_validation_classification_label.append(validation_batch_classification[i].item())\n","\n","                # Calculate validation F1 score with class weights for balance\n","                val_total_f1_score_value = f1_score(y_validation_classification_label, y_hat_validation_classification_label, average='macro')\n","                log['meta_model_validation_f1_score_percent'] = val_total_f1_score_value*100\n","\n","                # Calculate validation loss.\n","                val_loss_d = model.distance_loss() * model.distance_loss_weight\n","                val_loss_s = model.span_loss(y_hat_validation_projection, y_validation_classification_label) * model.span_loss_weight\n","                val_loss = val_loss_d + val_loss_s\n","\n","                log['model_validation_loss'] = val_loss.item()\n","                log['model_validation_loss_d'] = val_loss_d.item()\n","                log['model_validation_loss_s'] = val_loss_s.item()\n","                print(f\"\\n==== Epoch {epoch+1}, F1 score {val_total_f1_score_value*100:.4f}%, Validation loss {val_loss:.4f} ====\\n\")\n","\n","                if val_total_f1_score_value > local_maxima_f1_score:\n","                    local_maxima_f1_score = val_total_f1_score_value\n","                    local_maxima_meta_model_state = model.model.state_dict()\n","                    local_maxima_meta_length_embeddings = model.length_embeddings.detach().clone()\n","                    patience_counter = 0\n","                    if local_maxima_f1_score > best_f1_score:\n","                        best_f1_score = local_maxima_f1_score\n","                        local_maxima_f1_score = 0\n","                        best_meta_model_state = local_maxima_meta_model_state\n","                        best_meta_length_embeddings = local_maxima_meta_length_embeddings.to(device)\n","                else:\n","                    patience_counter += 1\n","                    if patience_counter >= model.adapt_patience:\n","                        break\n","\n","        logs.append(log)\n","\n","    if best_meta_model_state is not None and best_meta_length_embeddings is not None:\n","        model.model.load_state_dict(best_meta_model_state)\n","        model.length_embeddings.data = best_meta_length_embeddings.to(device)\n","    else:\n","        # Fallback to latest model if no improvement found\n","        model.model.load_state_dict(local_maxima_meta_model_state)\n","        model.length_embeddings.data = local_maxima_meta_length_embeddings.to(device)\n","\n","    log_df = pd.DataFrame(logs)\n","    hidden_dims_name = \"-\".join(map(str, hidden_dims))\n","    log_df.to_csv(f'{dir}/{model.shot_sample_number}_shot/{input_embedding_mode}_logs/improved_log_{hidden_dims_name}_tau_{model.tau}_validation_num_{model.validation_sample_number}.csv', index=False)\n","\n","    return model"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"4bcWFHseHcob","executionInfo":{"status":"error","timestamp":1746545595628,"user_tz":-330,"elapsed":3682,"user":{"displayName":"abigil","userId":"08750684783932887331"}},"outputId":"2bf2a116-2130-4836-e8a6-6271c6cf0ddb"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMlooEk1yHcC"},"outputs":[],"source":["# Independent variable:\n","    # shot number per each support set\n","    # types of input embedding\n","# Dependent variable:\n","    # modified davies bouldin index (MDBI)\n","    # F1 score\n","    # F1 score per each classification category.\n","# Control variable:\n","    # dropout (0.5)\n","    # activation function (gelu)\n","    # random seed for layer initiation (42)\n","    # output layer dimension (1024)\n","    # number of hidden layers (1)\n","# Train stop condition: when validation loss increases more than patience number\n","\n","# Embedding mode to input dim map\n","embedding_mode_to_input_dim = {'bert': 768, 'fasttext': 300}\n","\n","# Set hyperparameters.\n","adapt_epochs = 50\n","input_embedding_mode = 'bert' ##\n","shot_sample_number = 1 ##\n","validation_sample_number = 20\n","query_sample_number = 15 ## delete this later\n","number_of_hidden_layers = 1\n","\n","# Split test set and task set.\n","total_type_id_list_of_text_span = {}\n","for i in range(len(type_ids)):\n","    total_type_id_list_of_text_span[i] = []\n","for text_span in generated_text_span:\n","    if text_span.strip() == \"\":\n","        continue\n","    type_id = 0\n","    try:\n","        type_id = type_ids.index(named_entity_to_type_id[text_span])\n","    except:\n","        type_id = type_ids.index('UnknownType')\n","    total_type_id_list_of_text_span[type_id].append(text_span)\n","\n","support_set_type_id_list_of_text_span = {}\n","validation_set_type_id_list_of_text_span = {}\n","query_set_type_id_list_of_text_span = {}\n","\n","for key in total_type_id_list_of_text_span.keys():\n","    shuffled = total_type_id_list_of_text_span[key]\n","    random.shuffle(shuffled)\n","    support_set_type_id_list_of_text_span[key] = shuffled[:shot_sample_number]\n","    validation_set_type_id_list_of_text_span[key] = shuffled[shot_sample_number:shot_sample_number+validation_sample_number]\n","    query_set_type_id_list_of_text_span[key] = shuffled[shot_sample_number+validation_sample_number:shot_sample_number+validation_sample_number+query_sample_number]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clfRt5_CyGJo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746545818244,"user_tz":-330,"elapsed":207503,"user":{"displayName":"abigil","userId":"08750684783932887331"}},"outputId":"e27b1547-8a8d-4729-9929-8469050fa155"},"outputs":[{"output_type":"stream","name":"stdout","text":["Result of initial prototypical network training loss: 0.0\n","Epoch 1, Support loss 2.9832\n","\n","==== Epoch 1, F1 score 7.7806%, Validation loss 4.6021 ====\n","\n","Epoch 2, Support loss 0.0043\n","\n","==== Epoch 2, F1 score 8.3731%, Validation loss 4.8376 ====\n","\n","Epoch 3, Support loss 0.3882\n","\n","==== Epoch 3, F1 score 8.1585%, Validation loss 5.1661 ====\n","\n","Epoch 4, Support loss 0.0000\n","\n","==== Epoch 4, F1 score 11.5916%, Validation loss 5.1236 ====\n","\n","Epoch 5, Support loss 0.0000\n","\n","==== Epoch 5, F1 score 8.8469%, Validation loss 5.2875 ====\n","\n","Epoch 6, Support loss 0.2844\n","\n","==== Epoch 6, F1 score 9.9181%, Validation loss 5.2122 ====\n","\n","Epoch 7, Support loss 0.0000\n","\n","==== Epoch 7, F1 score 10.3230%, Validation loss 5.1252 ====\n","\n","Epoch 8, Support loss 0.0075\n","\n","==== Epoch 8, F1 score 11.1667%, Validation loss 5.1285 ====\n","\n","Epoch 9, Support loss 0.0014\n","\n","==== Epoch 9, F1 score 11.2769%, Validation loss 5.3986 ====\n","\n","Epoch 10, Support loss 0.0001\n","\n","==== Epoch 10, F1 score 9.7707%, Validation loss 5.7818 ====\n","\n","Epoch 11, Support loss 0.0201\n","\n","==== Epoch 11, F1 score 10.3102%, Validation loss 5.9103 ====\n","\n","Epoch 12, Support loss 0.0000\n","\n","==== Epoch 12, F1 score 9.3895%, Validation loss 6.0603 ====\n","\n","Epoch 13, Support loss 0.0000\n","\n","==== Epoch 13, F1 score 8.8250%, Validation loss 6.2275 ====\n","\n","Epoch 14, Support loss 0.0107\n","\n","==== Epoch 14, F1 score 9.3322%, Validation loss 6.2938 ====\n","\n","Epoch 15, Support loss 0.0037\n","\n","==== Epoch 15, F1 score 8.5335%, Validation loss 6.3519 ====\n","\n","Epoch 16, Support loss 0.0000\n","\n","==== Epoch 16, F1 score 8.0401%, Validation loss 6.4108 ====\n","\n","Epoch 17, Support loss 0.0011\n","\n","==== Epoch 17, F1 score 8.2411%, Validation loss 6.4349 ====\n","\n","Epoch 18, Support loss 0.0000\n","\n","==== Epoch 18, F1 score 8.8177%, Validation loss 6.4894 ====\n","\n","Epoch 19, Support loss 0.0001\n","\n","==== Epoch 19, F1 score 8.9069%, Validation loss 6.5241 ====\n","\n","Epoch 20, Support loss 0.0000\n","\n","==== Epoch 20, F1 score 8.8024%, Validation loss 6.5700 ====\n","\n","Epoch 21, Support loss 0.0001\n","\n","==== Epoch 21, F1 score 8.1200%, Validation loss 6.6133 ====\n","\n","Epoch 22, Support loss 0.1005\n","\n","==== Epoch 22, F1 score 9.4449%, Validation loss 6.6539 ====\n","\n","Epoch 23, Support loss 0.0008\n","\n","==== Epoch 23, F1 score 10.5062%, Validation loss 6.6873 ====\n","\n","Epoch 24, Support loss 0.0000\n","\n","==== Epoch 24, F1 score 11.0230%, Validation loss 6.7293 ====\n","\n","Epoch 25, Support loss 0.0665\n","\n","==== Epoch 25, F1 score 11.4042%, Validation loss 6.6445 ====\n","\n","Epoch 26, Support loss 0.0000\n","\n","==== Epoch 26, F1 score 11.1311%, Validation loss 6.6030 ====\n","\n","Epoch 27, Support loss 0.0020\n","\n","==== Epoch 27, F1 score 11.1625%, Validation loss 6.5932 ====\n","\n","Epoch 28, Support loss 0.0003\n","\n","==== Epoch 28, F1 score 11.0756%, Validation loss 6.6201 ====\n","\n","Epoch 29, Support loss 0.0004\n","\n","==== Epoch 29, F1 score 11.8044%, Validation loss 6.6615 ====\n","\n","Epoch 30, Support loss 0.0000\n","\n","==== Epoch 30, F1 score 11.2473%, Validation loss 6.7247 ====\n","\n","Epoch 31, Support loss 0.0000\n","\n","==== Epoch 31, F1 score 11.0504%, Validation loss 6.8016 ====\n","\n","Epoch 32, Support loss 0.0000\n","\n","==== Epoch 32, F1 score 10.7273%, Validation loss 6.8583 ====\n","\n","Epoch 33, Support loss 0.0000\n","\n","==== Epoch 33, F1 score 10.3581%, Validation loss 6.9327 ====\n","\n","Epoch 34, Support loss 0.0000\n","\n","==== Epoch 34, F1 score 10.5061%, Validation loss 7.0012 ====\n","\n","Epoch 35, Support loss 0.0000\n","\n","==== Epoch 35, F1 score 10.5049%, Validation loss 7.0797 ====\n","\n","Epoch 36, Support loss 0.0000\n","\n","==== Epoch 36, F1 score 10.1839%, Validation loss 7.1467 ====\n","\n","Epoch 37, Support loss 0.0000\n","\n","==== Epoch 37, F1 score 11.0882%, Validation loss 7.2040 ====\n","\n","Epoch 38, Support loss 0.0016\n","\n","==== Epoch 38, F1 score 10.8842%, Validation loss 7.2745 ====\n","\n","Epoch 39, Support loss 0.0007\n","\n","==== Epoch 39, F1 score 10.5332%, Validation loss 7.3116 ====\n","\n","Epoch 40, Support loss 0.0000\n","\n","==== Epoch 40, F1 score 10.5009%, Validation loss 7.3580 ====\n","\n","Epoch 41, Support loss 0.0000\n","\n","==== Epoch 41, F1 score 10.3631%, Validation loss 7.3967 ====\n","\n","Epoch 42, Support loss 0.0001\n","\n","==== Epoch 42, F1 score 10.4744%, Validation loss 7.4268 ====\n","\n","Epoch 43, Support loss 0.0000\n","\n","==== Epoch 43, F1 score 10.4728%, Validation loss 7.4756 ====\n","\n","Epoch 44, Support loss 0.0234\n","\n","==== Epoch 44, F1 score 10.2548%, Validation loss 7.4698 ====\n","\n","Epoch 45, Support loss 0.0002\n","\n","==== Epoch 45, F1 score 10.4366%, Validation loss 7.4970 ====\n","\n","Epoch 46, Support loss 0.0000\n","\n","==== Epoch 46, F1 score 10.2376%, Validation loss 7.5213 ====\n","\n","Epoch 47, Support loss 0.0000\n","\n","==== Epoch 47, F1 score 9.8554%, Validation loss 7.5254 ====\n","\n","Epoch 48, Support loss 0.0000\n","\n","==== Epoch 48, F1 score 10.0495%, Validation loss 7.5649 ====\n","\n","Epoch 49, Support loss 0.0055\n","\n","==== Epoch 49, F1 score 9.5751%, Validation loss 7.5868 ====\n","\n","Epoch 50, Support loss 0.0000\n","\n","==== Epoch 50, F1 score 9.4380%, Validation loss 7.5965 ====\n","\n"]}],"source":["layer_scale = [1024]*number_of_hidden_layers\n","results_list = []\n","\n","tau = 5.0 ##\n","\n","params = {\n","    'dropout': 0.5,\n","    'num_classes': len(type_ids),\n","    'additional_num_of_unknown_type': 10,\n","    'input_dim': embedding_mode_to_input_dim[input_embedding_mode],\n","    'projection_embedding_dim': 1024,\n","    'prototype_train_learning_rate': 0.2,\n","    'prototype_train_epochs': 1000,\n","    'prototype_train_patience': 5,\n","    'max_span_length': 10,\n","    'length_embedding_dim': 25,\n","    'shot_sample_number': shot_sample_number,\n","    'validation_sample_number': validation_sample_number,\n","    'adapt_learning_rate': 5e-3,\n","    'adapt_epochs': adapt_epochs,\n","    'adapt_patience': 100,\n","    'batch_size': 32,\n","    'tau': tau,\n","}\n","\n","# Train\n","model = adapt_train(\n","    model_class=EPNet,\n","    params=params,\n","    support_type_id_list_of_text_span=support_set_type_id_list_of_text_span,\n","    validation_type_id_list_of_text_span=validation_set_type_id_list_of_text_span,\n","    input_embedding_mode=input_embedding_mode,\n","    hidden_dims=layer_scale,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLpKm7nbz1W4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746545820990,"user_tz":-330,"elapsed":2748,"user":{"displayName":"abigil","userId":"08750684783932887331"}},"outputId":"f3f29b17-7ddd-4d4a-e005-c1052468dd8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total F1 score: 10.001984045993057%, Modified Davies Bouldin Index: 1.4364369419588623\n","F1 score per type: {'UnknownType': 0.0, 'Idea or Concept': 0.0125, 'Substance': 0.09090909090909091, 'Natural Phenomenon or Process': 0.06015037593984962, 'Occupational Activity': 0.0, 'Anatomical Structure': 0.03361344537815126, 'Finding': 0.0, 'Organism': 0.08421052631578947, 'Intellectual Product': 0.0, 'Manufactured Object': 0.0, 'Group': 0.4230769230769231, 'Activity': 0.0, 'Organism Attribute': 0.05263157894736842, 'Phenomenon or Process': 0.0, 'Behavior': 0.0, 'Injury or Poisoning': 0.0, 'Organization': 0.2, 'Conceptual Entity': 0.0, 'Occupation or Discipline': 0.0}\n"]}],"source":["# Recognizetotal_f1_score_value, f1_score_per_type_id, mdbi = model.recognize(query_set_type_id_list_of_text_span, input_embedding_mode)\n","total_f1_score_value, f1_score_per_type_id, mdbi = model.recognize(query_set_type_id_list_of_text_span, input_embedding_mode)\n","print(f\"Total F1 score: {total_f1_score_value*100}%, Modified Davies Bouldin Index: {mdbi}\")\n","print(f\"F1 score per type: {f1_score_per_type_id}\")\n","\n","result = {\n","    'number_of_hidden_layers': number_of_hidden_layers,\n","    'hidden_dims': \"-\".join(map(str, layer_scale)),\n","    'total_f1_score_precent': total_f1_score_value*100,\n","    'mdbi': mdbi,\n","}\n","for type_id, f1_score_value in f1_score_per_type_id.items():\n","    result[f'f1_score_{type_id}_percent'] = f1_score_value*100\n","results_list.append(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ujqm52Dz2qD"},"outputs":[],"source":["# Save model.\n","model_filename = model.get_model_filename(layer_scale)\n","torch.save(model.model.state_dict(), f'{dir}/{model.shot_sample_number}_shot/{input_embedding_mode}_models/ablated_model_{model_filename}')\n","torch.save(model.length_embeddings, f'{dir}/{model.shot_sample_number}_shot/{input_embedding_mode}_models/ablated_length_embeddings_{model_filename}')\n","torch.save(model.prototypes, f'{dir}/{model.shot_sample_number}_shot/{input_embedding_mode}_models/ablated_prototypes_{model_filename}')\n","\n","# Save performance results.\n","results = pd.DataFrame(results_list)\n","results.to_csv(f'{dir}/{model.shot_sample_number}_shot/{input_embedding_mode}_results/ablated_performance_result_tau_{tau}.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIKKLleEOlF0"},"outputs":[],"source":["from google.colab import runtime\n","\n","runtime.unassign()"]},{"cell_type":"code","source":[],"metadata":{"id":"1rWw-hU7J_l1"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":0}