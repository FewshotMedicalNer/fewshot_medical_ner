{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"elm9Z6CjC8W-"},"outputs":[],"source":["!pip install -q fasttext\n","\n","import math\n","import json\n","import copy\n","import pandas as pd\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score\n","from sklearn.manifold import MDS\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from transformers import AutoTokenizer, BertModel\n","import fasttext\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.colors import LinearSegmentedColormap\n","import gc\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["dir = ''"],"metadata":{"id":"MPFDmgiFKOvv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOPODUxdqSlN"},"outputs":[],"source":["# Call device.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initiate fixed random seed, for consistent and reproducible output.\n","def set_seed(seed: int = 42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)\n","\n","# Load type_ids.\n","with open(f'{dir}/type_ids.json', 'r') as f:\n","    type_ids = json.load(f)\n","unknown_index = type_ids.index(\"UnknownType\")\n","\n","# Set where to ablate\n","ablations = [\"hard_negative\", \"multiple_prototype\", \"contrastive_learning\"]\n","ablation = ablations[2] ##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GsuXnRCxqEL"},"outputs":[],"source":["# Load language model.\n","def load_language_model(input_embedding_mode):\n","    # Initiate language model. The language model will not be fine-tuned to avoid overfitting.\n","    if input_embedding_mode == 'bert':\n","        # Load vanilla bert-base-cased model for input embedding.\n","        tokenizer = AutoTokenizer.from_pretrained(f'{dir}/language_model/{input_embedding_mode}/tokenizer')\n","        pretrained_model = BertModel.from_pretrained(f'{dir}/language_model/{input_embedding_mode}/transformer')\n","        pretrained_model.eval()\n","        pretrained_model.to(device)\n","        pretrained_model.resize_token_embeddings(tokenizer.vocab_size)\n","\n","    elif input_embedding_mode == 'fasttext':\n","        # Load fasttext model for input embedding.\n","        pretrained_model = fasttext.load_model(f'{dir}/language_model/{input_embedding_mode}/cc.en.300.bin')\n","        tokenizer = None\n","\n","    return (tokenizer, pretrained_model)\n","\n","# Define model class.\n","class ReProCon(nn.Module):\n","    def __init__(self, params, ffn_hidden_dims, input_embedding_mode, type_ids):\n","        super().__init__()\n","\n","        # Receive hyperparameters.\n","        self.train_test_split_ratio = params['train_test_split_ratio']\n","        self.num_classes = params['num_classes']\n","        self.dropout = params['dropout']\n","        self.input_dim = params['input_dim']\n","        self.max_input_tokens_length = params['max_input_tokens_length']\n","        self.positional_embedding_dim = params['positional_embedding_dim']\n","        self.bilstm_hidden_dim = params['bilstm_hidden_dim']\n","        self.bilstm_layers = params['bilstm_layers']\n","        self.lstm_embedding_dim = params['lstm_embedding_dim']\n","        self.projection_embedding_dim = params['projection_embedding_dim']\n","        self.temp = params['temp']\n","        self.prototype_train_learning_rate = params['prototype_train_learning_rate']\n","        self.prototype_train_epochs = params['prototype_train_epochs']\n","        self.prototype_train_patience = params['prototype_train_patience']\n","        self.prototype_num_per_class = params['prototype_num_per_class']\n","        self.shot_sample_number = params['shot_sample_number']\n","        self.meta_learning_rate = params['meta_learning_rate']\n","        self.task_learning_rate = params['task_learning_rate']\n","        self.meta_epochs = params['meta_epochs']\n","        self.task_epochs = params['task_epochs']\n","        self.adapt_patience = params['adapt_patience']\n","        self.batch_size = params['batch_size']\n","        self.input_embedding_mode = input_embedding_mode\n","        self.type_ids = type_ids\n","        self.prototype_loss_weight = params['prototype_loss_weight']\n","\n","        # Positional embedding\n","        self.set_positional_embedding()\n","\n","        # BiLSTM model\n","        self.bilstm_model = nn.LSTM(\n","            input_size=self.input_dim+self.positional_embedding_dim,\n","            hidden_size=self.bilstm_hidden_dim,\n","            num_layers=self.bilstm_layers,\n","            bias=True,\n","            batch_first=True,\n","            dropout=self.dropout,\n","            bidirectional=True,\n","            proj_size=self.lstm_embedding_dim, # Since it is bidirectional, it becomes *2\n","            device=device\n","        )\n","\n","        # FFN model\n","        ffn_layers = []\n","        ffn_dims = [self.input_dim if self.input_embedding_mode == 'bert' else 2*self.lstm_embedding_dim]\n","        ffn_dims.extend(ffn_hidden_dims)\n","        for i in range(len(ffn_hidden_dims)):\n","            ffn_layers.append(nn.Linear(ffn_dims[i], ffn_dims[i+1]))\n","            ffn_layers.append(nn.BatchNorm1d(ffn_dims[i+1]))\n","            ffn_layers.append(nn.GELU())\n","            ffn_layers.append(nn.Dropout(self.dropout))\n","        ffn_layers.append(nn.Linear(ffn_dims[-1], self.projection_embedding_dim))\n","        ffn_layers.append(nn.LayerNorm(self.projection_embedding_dim))\n","        self.ffn_model = nn.Sequential(*ffn_layers)\n","\n","        prototypes = np.random.rand(self.num_classes*self.prototype_num_per_class, self.projection_embedding_dim)\n","        self.prototypes = torch.tensor(prototypes, dtype=torch.float32)\n","        self.prototypes.data = F.normalize(self.prototypes.data, dim=1)\n","        self.prototypes.requires_grad_(True)\n","\n","    def set_positional_embedding(self):\n","        pos = torch.arange(0, self.max_input_tokens_length).unsqueeze(1)\n","        cols = torch.arange(0, self.positional_embedding_dim).unsqueeze(0)\n","        position_tensor = pos / (torch.pow(10000, (2*(cols//2)) / self.positional_embedding_dim))\n","\n","        position_tensor[:, 0::2] = torch.sin(position_tensor[:, 0::2])\n","        position_tensor[:, 1::2] = torch.cos(position_tensor[:, 1::2])\n","\n","        self.positional_embedding = position_tensor.to(device)\n","\n","    def input_embed(self, sample, language_model):\n","        mark_index = sample[0].index(\"[MARK_POSITION]\")\n","        full_sentence = sample[0].copy()\n","        full_sentence[mark_index] = sample[1]\n","\n","        if self.input_embedding_mode == 'bert':\n","            tokenizer, pretrained_model = language_model\n","            named_entity_start_index = 1\n","            named_entity_end_index = 1\n","            for i in range(len(full_sentence)):\n","                if i == mark_index:\n","                    named_entity_encodings = tokenizer.encode(full_sentence[i])\n","                    named_entity_tokens = tokenizer.convert_ids_to_tokens(named_entity_encodings)\n","                    named_entity_tokens.remove(\"[CLS]\")\n","                    named_entity_tokens.remove(\"[SEP]\")\n","                    named_entity_end_index = named_entity_start_index + len(named_entity_tokens)\n","                    break\n","                else:\n","                    not_named_entity_encodings = tokenizer.encode(full_sentence[i])\n","                    not_named_entity_tokens = tokenizer.convert_ids_to_tokens(not_named_entity_encodings)\n","                    not_named_entity_tokens.remove(\"[CLS]\")\n","                    not_named_entity_tokens.remove(\"[SEP]\")\n","                    named_entity_start_index += len(not_named_entity_tokens)\n","\n","            full_sentence_text = ' '.join(full_sentence)\n","            full_sentence_encodings = tokenizer(\n","                full_sentence_text,\n","                max_length=self.max_input_tokens_length,\n","                return_tensors='pt',\n","                padding=\"max_length\",\n","                truncation=True,\n","            )\n","            full_sentence_encodings = {k: v.to(device) for k, v in full_sentence_encodings.items()}\n","            with torch.no_grad():\n","                outputs = pretrained_model(**full_sentence_encodings)\n","                named_entity_embedding = outputs.last_hidden_state[:, named_entity_start_index:named_entity_end_index, :]\n","                if torch.cuda.is_available():\n","                    torch.cuda.synchronize()\n","                max_pool_tensor, _ = torch.max(named_entity_embedding, dim=1)\n","                mean_pool_tensor = torch.mean(named_entity_embedding, dim=1)\n","                span_representation = (max_pool_tensor+mean_pool_tensor)/2\n","\n","                return span_representation, None\n","\n","        elif self.input_embedding_mode == 'fasttext':\n","            _tokenizer, pretrained_model = language_model\n","            forwarded_embedding = torch.zeros(self.max_input_tokens_length, self.input_dim)\n","            for i in range(len(full_sentence)):\n","                tokens = full_sentence[i].split(' ')\n","                if len(tokens) > 1:\n","                    normalized_embeddings = []\n","                    for token in tokens:\n","                        token_vector = pretrained_model.get_word_vector(token)\n","                        norm = np.sqrt(np.sum(token_vector**2))\n","                        if not norm == 0:\n","                            normalized_embeddings.append(token_vector/norm)\n","                        else:\n","                            normalized_embeddings.append(token_vector)\n","\n","                    # Use weighted averaging for better representation\n","                    if len(normalized_embeddings) > 0:\n","                        weights = np.array([1.0 + 0.1*(i-len(normalized_embeddings)/2)**2 for i in range(len(normalized_embeddings))])\n","                        weights = weights / weights.sum()\n","                        mean_vector = np.average(normalized_embeddings, axis=0, weights=weights)\n","                    else:\n","                        mean_vector = np.zeros(self.input_dim)\n","                    span_representation = torch.from_numpy(mean_vector).unsqueeze(0).float().to(device)\n","                    forwarded_embedding[i] = span_representation\n","                else:\n","                    token_vector = pretrained_model.get_word_vector(tokens[0])\n","                    norm = np.sqrt(np.sum(token_vector**2))\n","                    if not norm == 0:\n","                        token_vector = token_vector/norm\n","                    else:\n","                        token_vector = token_vector\n","                    span_representation = torch.from_numpy(token_vector).unsqueeze(0).float().to(device)\n","                    forwarded_embedding[i] = span_representation\n","\n","            forwarded_embedding = forwarded_embedding.to(device)\n","            # This embedding will be forwarded to BiLSTM model. This is because fasttext itself does not reflect position information.\n","            # When embedding via BERT, adding positional embedding and forwarding BiLSTM will be skipped, since BERT model itself already includes this process.\n","            concatenated_embedding = torch.cat((forwarded_embedding, self.positional_embedding), dim=1)\n","\n","            return concatenated_embedding, mark_index\n","\n","    def forward(self, input, mark_index_list):\n","        projection = torch.empty((0, self.projection_embedding_dim))\n","        projection = projection.to(device)\n","        if self.input_embedding_mode == 'bert':\n","            projection = torch.cat((projection, self.ffn_model.forward(input)), dim=0)\n","\n","        elif self.input_embedding_mode == 'fasttext':\n","            lstm_projection = torch.zeros(len(mark_index_list), self.lstm_embedding_dim*2)\n","            bilstm_forwarded_sequence, _ = self.bilstm_model(input)\n","            for i in range(len(mark_index_list)):\n","                mark_index = mark_index_list[i]\n","                projection_tensor = bilstm_forwarded_sequence[i][mark_index]\n","                lstm_projection[i] = projection_tensor\n","            lstm_projection = lstm_projection.to(device)\n","\n","            projection = torch.cat((projection, self.ffn_model.forward(lstm_projection)), dim=0)\n","\n","        self.prototypes = self.prototypes.to(device)\n","\n","        projection = F.normalize(projection, p=2, dim=1)\n","        prototypes_normalized = self.prototypes\n","        similarity_matrix = torch.mm(projection, prototypes_normalized.t())\n","        square_similarity_matrix = -(1-similarity_matrix)*(1-similarity_matrix)\n","        logits = torch.softmax(square_similarity_matrix/self.temp, dim=1)\n","        classification_result_list = torch.argmax(logits, dim=1).tolist()\n","        for i in range(len(classification_result_list)):\n","            classification_result_list[i] = classification_result_list[i] // self.prototype_num_per_class\n","\n","        return projection, logits, classification_result_list\n","\n","    def prototype_loss(self):\n","        prototypes = self.prototypes.to(device)\n","        prototypes = F.normalize(prototypes, p=2, dim=1)\n","        num_types = prototypes.size(0)\n","\n","        # Code from https://github.com/psmmettes/hpn\n","        # Use this to minimize mean maximum cosine similarity\n","        similarity = torch.mm(prototypes, prototypes.t())+1\n","        similarity -= 2.0 * torch.diag(torch.diag(similarity))\n","        loss = similarity.max(dim=1)[0].mean()\n","\n","        return loss\n","\n","    # Improved span loss function using supervised contrastive learning loss\n","    def span_loss(self, y_hat_projection, y_classification_label):\n","        y_classification_label_copy = y_classification_label.copy()\n","        y_classification_label_copy = torch.Tensor(y_classification_label_copy).long().to(device)\n","        # Code from https://arxiv.org/abs/2004.11362\n","        y_hat_projection_normalized = F.normalize(y_hat_projection, p=2, dim=1)\n","        y_hat_projection_normalized = y_hat_projection_normalized.to(device)\n","        prototypes_normalized = self.prototypes.to(device)\n","        similarity_matrix = torch.mm(prototypes_normalized, y_hat_projection_normalized.t())\n","\n","        # Supervised Contrastive Learning\n","        if ablation != \"contrastive_learning\":\n","            # Code from https://arxiv.org/abs/1901.10514\n","            square_similarity_matrix = (1-similarity_matrix)*(1-similarity_matrix)\n","            # Minimum pooling per each category\n","            pooled_square_similarity_matrix = torch.zeros(self.num_classes, len(y_classification_label))\n","            for i in range(self.num_classes):\n","                square_similarity_matrix_per_class = square_similarity_matrix[i*self.prototype_num_per_class:(i+1)*self.prototype_num_per_class, :]\n","                pooled_square_similarity_matrix[i] = square_similarity_matrix_per_class.min(dim=0)[0]\n","            pooled_square_similarity_matrix = pooled_square_similarity_matrix.to(device)\n","\n","            mask = torch.zeros(self.num_classes, len(y_classification_label), dtype=torch.bool)\n","            for i in range(len(y_classification_label)):\n","                mask[y_classification_label[i]][i] = True\n","            mask = mask.to(device)\n","            masked_square_similarity_matrix = pooled_square_similarity_matrix*mask\n","\n","            positive_mask_counts = mask.sum(dim=1).float()\n","\n","            # Since anchor is a prototype not a sample, skip removing auto-similarities\n","            similarity_sum = torch.sum(pooled_square_similarity_matrix, dim=1)\n","            positive_similarity_sum = torch.sum(masked_square_similarity_matrix, dim=1)\n","            positive_similarity_mean = positive_similarity_sum / positive_mask_counts\n","            contrastive_loss_per_class = positive_similarity_mean / similarity_sum\n","            loss = torch.sum(contrastive_loss_per_class, dim=0)\n","            loss = loss.to(device)\n","        else:\n","            for i in range(self.num_classes):\n","                class_sample_range = (self.shot_sample_number*i, self.shot_sample_number*(i+1))\n","                class_prototype_range = (self.prototype_num_per_class*i, self.prototype_num_per_class*(i+1))\n","                class_similarity_matrix = similarity_matrix[class_prototype_range[0]:class_prototype_range[1], class_sample_range[0]:class_sample_range[1]]\n","                maximum_similarity_indices = torch.argmax(class_similarity_matrix, dim=0)\n","                maximum_similarity_indices += class_sample_range[0]\n","                y_classification_label_copy[class_sample_range[0]:class_sample_range[1]] = maximum_similarity_indices\n","            logit = torch.softmax(similarity_matrix, dim=1).t()\n","            loss = F.cross_entropy(logit, y_classification_label_copy)\n","\n","        return loss\n","\n","    def select_hard_negatives(self, task_set_type_id_list_of_samples, language_model):\n","        self.eval()\n","\n","        hard_negative_set_type_id_list_of_samples = {}\n","\n","        for key, samples in tqdm(task_set_type_id_list_of_samples.items()):\n","            collect = []\n","\n","            if self.input_embedding_mode == 'bert':\n","                test_batch_input = torch.zeros(\n","                    self.batch_size,\n","                    self.input_dim\n","                )\n","            elif self.input_embedding_mode == 'fasttext':\n","                test_batch_input = torch.zeros(\n","                    self.batch_size,\n","                    self.max_input_tokens_length,\n","                    self.input_dim+self.positional_embedding_dim\n","                )\n","            test_batch_mark_index_list = []\n","            intra_batch_count = 0\n","            multiple_count = 0\n","            for sample in task_set_type_id_list_of_samples[key]:\n","                input_embedding, mark_index = self.input_embed(sample, language_model)\n","                test_batch_input[intra_batch_count] = input_embedding\n","                if self.input_embedding_mode == 'fasttext':\n","                    test_batch_mark_index_list.append(mark_index)\n","                intra_batch_count += 1\n","                if intra_batch_count == self.batch_size:\n","                    test_batch_input = test_batch_input.to(device)\n","                    _, _, test_batch_classification = self.forward(test_batch_input, test_batch_mark_index_list)\n","                    for i in range(intra_batch_count):\n","                        if test_batch_classification[i] != int(key):\n","                            collect.append(samples[multiple_count*self.batch_size+i])\n","\n","                    intra_batch_count = 0\n","                    multiple_count += 1\n","\n","                    del test_batch_classification\n","                    gc.collect()\n","\n","                    if self.input_embedding_mode == 'bert':\n","                        test_batch_input = torch.zeros(\n","                            self.batch_size,\n","                            self.input_dim\n","                        )\n","                    elif self.input_embedding_mode == 'fasttext':\n","                        test_batch_input = torch.zeros(\n","                            self.batch_size,\n","                            self.max_input_tokens_length,\n","                            self.input_dim+self.positional_embedding_dim\n","                        )\n","                    test_batch_mark_index_list = []\n","            if intra_batch_count != 0:\n","                test_batch_input = test_batch_input[:intra_batch_count].to(device)\n","                _, _, test_batch_classification = self.forward(test_batch_input, test_batch_mark_index_list)\n","                for i in range(intra_batch_count):\n","                    if test_batch_classification[i] != int(key):\n","                        collect.append(samples[multiple_count*self.batch_size+i])\n","\n","            hard_negative_set_type_id_list_of_samples[int(key)] = collect\n","\n","            del test_batch_classification\n","            gc.collect()\n","\n","            # Empty cuda\n","            test_batch_input = test_batch_input.cpu()\n","            del test_batch_input\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        return hard_negative_set_type_id_list_of_samples\n","\n","# Total training process.\n","def train(model_class, meta_model, params, ffn_hidden_dims, input_embedding_mode, type_ids, tasks, validation_set_type_id_list_of_samples, language_model, is_initial):\n","    meta_model.to(device)\n","    meta_model.train()\n","\n","    # Assign random tasks.\n","    task_indices = list(range(len(tasks)))\n","    random.shuffle(task_indices)\n","\n","    logs = []\n","    patience_counter = 0\n","\n","    # F1 score would be better benchmark.\n","    best_f1_score = 0\n","    local_maxima_f1_score = 0\n","    best_meta_ffn_model_state = None\n","    local_maxima_meta_ffn_model_state = None\n","    if input_embedding_mode == 'fasttext':\n","        best_meta_lstm_model_state = None\n","        local_maxima_meta_lstm_model_state = None\n","\n","    for meta_epoch in range(meta_model.meta_epochs):\n","        log = {}\n","        log['meta_epoch'] = meta_epoch+1\n","\n","        # Bring task.\n","        support_set_type_id_list_of_samples = tasks[task_indices[meta_epoch]]\n","\n","        # Clone meta model.\n","        task_model=model_class(\n","            params=params,\n","            ffn_hidden_dims=ffn_hidden_dims,\n","            input_embedding_mode=input_embedding_mode,\n","            type_ids=type_ids,\n","        )\n","\n","        if input_embedding_mode == 'fasttext':\n","            task_model.bilstm_model.load_state_dict(meta_model.bilstm_model.state_dict())\n","        task_model.ffn_model.load_state_dict(meta_model.ffn_model.state_dict())\n","        task_model.prototypes.data = meta_model.prototypes.detach().clone().to(device)\n","        task_model.to(device)\n","        task_model.train()\n","\n","        if input_embedding_mode == 'bert':\n","            task_model_optimizer = optim.AdamW(\n","                params=list(task_model.ffn_model.parameters())+[task_model.prototypes],\n","                lr=meta_model.task_learning_rate,\n","                weight_decay=0.01\n","            )\n","        if input_embedding_mode == 'fasttext':\n","            task_model_optimizer = optim.AdamW(\n","                params=list(task_model.bilstm_model.parameters())+list(task_model.ffn_model.parameters())+[task_model.prototypes],\n","                lr=meta_model.task_learning_rate,\n","                weight_decay=0.01\n","            )\n","\n","        # Cosine annealing scheduler for better optimization.\n","        task_model_scheduler = optim.lr_scheduler.CosineAnnealingLR(task_model_optimizer, T_max=meta_model.task_epochs)\n","\n","        if input_embedding_mode == 'fasttext':\n","            old_task_bilstm_model_state = copy.deepcopy(task_model.bilstm_model.state_dict())\n","        old_task_ffn_model_state = copy.deepcopy(task_model.ffn_model.state_dict())\n","        old_task_prototypes_state = meta_model.prototypes.detach().clone().to(device)\n","\n","        for task_epoch in range(meta_model.task_epochs):\n","            task_model_optimizer.zero_grad()\n","\n","            y_support_classification_label = []\n","            for key in support_set_type_id_list_of_samples.keys():\n","                y_support_classification_label.extend([key]*len(support_set_type_id_list_of_samples[key]))\n","\n","            y_hat_support_projection = torch.zeros(\n","                len(y_support_classification_label),\n","                task_model.projection_embedding_dim\n","            )\n","            if input_embedding_mode == 'bert':\n","                support_batch_input = torch.zeros(\n","                    task_model.batch_size,\n","                    task_model.input_dim\n","                )\n","            elif input_embedding_mode == 'fasttext':\n","                support_batch_input = torch.zeros(\n","                    task_model.batch_size,\n","                    task_model.max_input_tokens_length,\n","                    task_model.input_dim+task_model.positional_embedding_dim\n","                )\n","            support_batch_mark_index_list = []\n","            intra_batch_count = 0\n","            multiple_count = 0\n","            for key in support_set_type_id_list_of_samples.keys():\n","                for sample in support_set_type_id_list_of_samples[key]:\n","                    input_embedding, mark_index = task_model.input_embed(sample, language_model)\n","                    support_batch_input[intra_batch_count] = input_embedding\n","                    if input_embedding_mode == 'fasttext':\n","                        support_batch_mark_index_list.append(mark_index)\n","                    intra_batch_count += 1\n","                    if intra_batch_count == task_model.batch_size:\n","                        support_batch_input = support_batch_input.to(device)\n","                        support_batch_projection, _, _ = task_model.forward(support_batch_input, support_batch_mark_index_list)\n","                        for i in range(intra_batch_count):\n","                            y_hat_support_projection[multiple_count*meta_model.batch_size+i] = support_batch_projection[i]\n","                        intra_batch_count = 0\n","                        multiple_count += 1\n","                        if input_embedding_mode == 'bert':\n","                            support_batch_input = torch.zeros(\n","                                task_model.batch_size,\n","                                task_model.input_dim\n","                            )\n","                        elif input_embedding_mode == 'fasttext':\n","                            support_batch_input = torch.zeros(\n","                                task_model.batch_size,\n","                                task_model.max_input_tokens_length,\n","                                task_model.input_dim+task_model.positional_embedding_dim\n","                            )\n","                        support_batch_mark_index_list = []\n","            if intra_batch_count != 0:\n","                support_batch_input = support_batch_input[:intra_batch_count].to(device)\n","                support_batch_projection, _, _ = task_model.forward(support_batch_input, support_batch_mark_index_list)\n","                for i in range(intra_batch_count):\n","                    y_hat_support_projection[multiple_count*meta_model.batch_size+i] = support_batch_projection[i]\n","\n","            # Calculate support loss with weighted components\n","            supp_loss_d = task_model.prototype_loss()*task_model.prototype_loss_weight\n","            supp_loss_s = task_model.span_loss(y_hat_support_projection, y_support_classification_label)\n","            supp_loss = supp_loss_d+supp_loss_s\n","            log[f'task_model_epoch_{task_epoch+1}_support_loss'] = supp_loss.item()\n","            log[f'task_model_epoch_{task_epoch+1}_support_loss_d'] = supp_loss_d.item()\n","            log[f'task_model_epoch_{task_epoch+1}_support_loss_s'] = supp_loss_s.item()\n","            log[f'task_model_epoch_{task_epoch+1}_task_learning_rate'] = task_model_optimizer.param_groups[0]['lr']\n","\n","            supp_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(task_model.parameters(), 1.0) # Add gradient clipping\n","            task_model_optimizer.step()\n","            task_model_scheduler.step()\n","            print(f\"Task epoch {task_epoch+1}, Support loss {supp_loss.item():.4f}\")\n","\n","        if input_embedding_mode == 'fasttext':\n","            new_task_bilstm_model_state = copy.deepcopy(task_model.bilstm_model.state_dict())\n","            new_meta_bilstm_model_state = meta_model.bilstm_model.state_dict()\n","\n","            with torch.no_grad():\n","                for name in new_meta_bilstm_model_state.keys():\n","                    if name in new_task_bilstm_model_state.keys():\n","                        if name in [n for n, _ in meta_model.bilstm_model.named_parameters()]:\n","                            new_meta_bilstm_model_state[name] += (new_task_bilstm_model_state[name]-old_task_bilstm_model_state[name])*meta_model.meta_learning_rate\n","                        else:\n","                            new_meta_bilstm_model_state[name] = new_task_bilstm_model_state[name]\n","\n","            meta_model.bilstm_model.load_state_dict(new_meta_bilstm_model_state)\n","\n","        new_task_ffn_model_state = copy.deepcopy(task_model.ffn_model.state_dict())\n","        new_meta_ffn_model_state = meta_model.ffn_model.state_dict()\n","\n","        with torch.no_grad():\n","            for name in new_meta_ffn_model_state.keys():\n","                if name in new_task_ffn_model_state.keys():\n","                    if name in [n for n, _ in meta_model.ffn_model.named_parameters()]:\n","                        new_meta_ffn_model_state[name] += (new_task_ffn_model_state[name]-old_task_ffn_model_state[name])*meta_model.meta_learning_rate\n","                    else:\n","                        new_meta_ffn_model_state[name] = new_task_ffn_model_state[name]\n","\n","        meta_model.ffn_model.load_state_dict(new_meta_ffn_model_state)\n","\n","        new_task_prototypes_state = task_model.prototypes.detach().clone().to(device)\n","        updated_prototypes = old_task_prototypes_state + (new_task_prototypes_state-old_task_prototypes_state)*meta_model.meta_learning_rate\n","        meta_model.prototypes.data = F.normalize(updated_prototypes, p=2, dim=1)\n","\n","        # Task validation\n","        meta_model.eval()\n","        with torch.no_grad():\n","            y_validation_classification_label = []\n","            for key in validation_set_type_id_list_of_samples.keys():\n","                y_validation_classification_label.extend([int(key)]*len(validation_set_type_id_list_of_samples[key]))\n","\n","            y_hat_validation_classification_label = []\n","            y_hat_validation_projection = torch.zeros(\n","                len(y_validation_classification_label),\n","                meta_model.projection_embedding_dim\n","            )\n","            if input_embedding_mode == 'bert':\n","                validation_batch_input = torch.zeros(\n","                    meta_model.batch_size,\n","                    meta_model.input_dim\n","                )\n","            elif input_embedding_mode == 'fasttext':\n","                validation_batch_input = torch.zeros(\n","                    meta_model.batch_size,\n","                    meta_model.max_input_tokens_length,\n","                    meta_model.input_dim+meta_model.positional_embedding_dim\n","                )\n","            validation_batch_mark_index_list = []\n","            intra_batch_count = 0\n","            multiple_count = 0\n","            for key in validation_set_type_id_list_of_samples.keys():\n","                for sample in validation_set_type_id_list_of_samples[key]:\n","                    input_embedding, mark_index = meta_model.input_embed(sample, language_model)\n","                    validation_batch_input[intra_batch_count] = input_embedding\n","                    if input_embedding_mode == 'fasttext':\n","                        validation_batch_mark_index_list.append(mark_index)\n","                    intra_batch_count += 1\n","                    if intra_batch_count == meta_model.batch_size:\n","                        validation_batch_input = validation_batch_input.to(device)\n","                        validation_batch_projection, _, validation_batch_classification = meta_model.forward(validation_batch_input, validation_batch_mark_index_list)\n","                        y_hat_validation_classification_label.extend(validation_batch_classification)\n","                        for i in range(intra_batch_count):\n","                            y_hat_validation_projection[multiple_count*meta_model.batch_size+i] = validation_batch_projection[i]\n","                        intra_batch_count = 0\n","                        multiple_count += 1\n","                        if input_embedding_mode == 'bert':\n","                            validation_batch_input = torch.zeros(\n","                                meta_model.batch_size,\n","                                meta_model.input_dim\n","                            )\n","                        elif input_embedding_mode == 'fasttext':\n","                            validation_batch_input = torch.zeros(\n","                                meta_model.batch_size,\n","                                meta_model.max_input_tokens_length,\n","                                meta_model.input_dim+meta_model.positional_embedding_dim\n","                            )\n","                        validation_batch_mark_index_list = []\n","            if intra_batch_count != 0:\n","                validation_batch_input = validation_batch_input[:intra_batch_count].to(device)\n","                validation_batch_projection, _, validation_batch_classification = meta_model.forward(validation_batch_input, validation_batch_mark_index_list)\n","                y_hat_validation_classification_label.extend(validation_batch_classification)\n","                for i in range(intra_batch_count):\n","                    y_hat_validation_projection[multiple_count*meta_model.batch_size+i] = validation_batch_projection[i]\n","\n","            # Calculate validation F1 score.\n","            y_validation_classification_label_tensor = torch.tensor(y_validation_classification_label)\n","            y_hat_validation_classification_label_tensor = torch.tensor(y_hat_validation_classification_label)\n","            val_total_f1_score_value = f1_score(y_validation_classification_label_tensor, y_hat_validation_classification_label_tensor, average='macro')\n","            log['meta_model_validation_f1_score_percent'] = val_total_f1_score_value*100\n","\n","            # Calculate validation loss.\n","            val_loss_d = meta_model.prototype_loss()*meta_model.prototype_loss_weight\n","            val_loss_s = meta_model.span_loss(y_hat_validation_projection, y_validation_classification_label)\n","            val_loss = val_loss_d+val_loss_s\n","\n","            log['meta_model_validation_loss'] = val_loss.item()\n","            log['meta_model_validation_loss_d'] = val_loss_d.item()\n","            log['meta_model_validation_loss_s'] = val_loss_s.item()\n","            print(f\"\\n==== Meta epoch {meta_epoch+1}, Validation F1 score {val_total_f1_score_value*100:.4f}%, Validation loss {val_loss.item():.4f} ====\\n\")\n","\n","            if val_total_f1_score_value > local_maxima_f1_score:\n","                local_maxima_f1_score = val_total_f1_score_value\n","                if input_embedding_mode == 'fasttext':\n","                    local_maxima_meta_lstm_model_state = meta_model.bilstm_model.state_dict()\n","                local_maxima_meta_ffn_model_state = meta_model.ffn_model.state_dict()\n","                patience_counter = 0\n","                if local_maxima_f1_score > best_f1_score:\n","                    best_f1_score = local_maxima_f1_score\n","                    local_maxima_f1_score = 0\n","                    if input_embedding_mode == 'fasttext':\n","                        best_meta_lstm_model_state = local_maxima_meta_lstm_model_state\n","                    best_meta_ffn_model_state = local_maxima_meta_ffn_model_state\n","                    if (1.0-local_maxima_f1_score) < 1e-8:\n","                        break\n","            else:\n","                patience_counter += 1\n","                if patience_counter >= meta_model.adapt_patience:\n","                    break\n","\n","        logs.append(log)\n","\n","    if input_embedding_mode == 'bert':\n","        if best_meta_ffn_model_state is not None:\n","            meta_model.ffn_model.load_state_dict(best_meta_ffn_model_state)\n","        else:\n","            meta_model.ffn_model.load_state_dict(local_maxima_meta_ffn_model_state)\n","\n","    elif input_embedding_mode == 'fasttext':\n","        if best_meta_lstm_model_state is not None and best_meta_ffn_model_state is not None:\n","            meta_model.bilstm_model.load_state_dict(best_meta_lstm_model_state)\n","            meta_model.ffn_model.load_state_dict(best_meta_ffn_model_state)\n","        else:\n","            # Fallback to latest model if no improvement found\n","            meta_model.bilstm_model.load_state_dict(local_maxima_meta_lstm_model_state)\n","            meta_model.ffn_model.load_state_dict(local_maxima_meta_ffn_model_state)\n","\n","    if input_embedding_mode == 'fasttext':\n","        del best_meta_lstm_model_state, local_maxima_meta_lstm_model_state\n","\n","    del best_meta_ffn_model_state, local_maxima_meta_ffn_model_state, new_task_ffn_model_state, old_task_ffn_model_state, new_task_prototypes_state, old_task_prototypes_state\n","    gc.collect()\n","\n","    # Clear every cache in scheduler\n","    for param in task_model_optimizer.state.values():\n","        if isinstance(param, torch.Tensor):\n","            param.data = param.data.cpu()\n","            if param._grad is not None:\n","                param._grad.data = param._grad.data.cpu()\n","        elif isinstance(param, dict):\n","            for subparam in param.values():\n","                if isinstance(subparam, torch.Tensor):\n","                    subparam.data = subparam.data.cpu()\n","                    if subparam._grad is not None:\n","                        subparam._grad.data = subparam._grad.data.cpu()\n","\n","    del task_model_optimizer, task_model_scheduler, task_model\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    log_df = pd.DataFrame(logs)\n","\n","    return log_df\n","\n","def make_task_set(task_set_type_id_list_of_samples, meta_epochs, shot_sample_number, num_classes):\n","    # Tasks are combinations of support set.\n","    type_id_combinations = {}\n","    for type_id, list_of_samples in tqdm(task_set_type_id_list_of_samples.items()):\n","        list_of_samples_copy = list_of_samples.copy()\n","        combinations = []\n","        while len(combinations) < meta_epochs:\n","            support_pairs = list(tuple(sorted(random.sample(list_of_samples_copy, shot_sample_number))))\n","            already_in_list = False\n","            for combination in combinations:\n","                if combination == support_pairs:\n","                    already_in_list = True\n","                    break\n","            if not already_in_list:\n","                combinations.append(support_pairs)\n","        type_id_combinations[int(type_id)] = combinations\n","\n","    tasks = []\n","    for i in range(meta_epochs):\n","        support_set_type_id_list_of_samples = {}\n","        for j in range(num_classes):\n","            support_set_type_id_list_of_samples[j] = type_id_combinations[j][i]\n","        tasks.append(support_set_type_id_list_of_samples)\n","\n","    return tasks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_NVYfZF8OHF"},"outputs":[],"source":["meta_epochs = 200\n","shot_sample_number = 5\n","task_sample_ratio = 0.3\n","input_embedding_mode = 'fasttext'\n","embedding_mode_to_input_dim = {'bert': 768, 'fasttext': 300}\n","ffn_hidden_dims = [1024]\n","language_model = load_language_model(input_embedding_mode)\n","\n","# Load task_set_type_id_list_of_samples\n","with open(f'{dir}/ablation_study/task_set_type_id_list_of_samples.json', 'r') as f:\n","    task_set_type_id_list_of_samples = json.load(f)\n","# Load validation_set_type_id_list_of_samples\n","with open(f'{dir}/ablation_study/validation_set_type_id_list_of_samples.json', 'r') as f:\n","    validation_set_type_id_list_of_samples = json.load(f)\n","\n","# Limit total number of samples to 30000, due to limitations of computing resources\n","for type_id, list_of_samples in task_set_type_id_list_of_samples.items():\n","    scrambled = list_of_samples.copy()\n","    random.shuffle(scrambled)\n","    task_set_type_id_list_of_samples[type_id] = scrambled[:30000]\n","# Limit total number of samples to 500, due to limitations of computing resources\n","for type_id, list_of_samples in validation_set_type_id_list_of_samples.items():\n","    scrambled = list_of_samples.copy()\n","    random.shuffle(scrambled)\n","    validation_set_type_id_list_of_samples[type_id] = scrambled[:500]\n","\n","if input_embedding_mode == 'bert':\n","    params = {\n","        'train_test_split_ratio': task_sample_ratio,\n","        'num_classes': len(type_ids),\n","        'dropout': 0.5,\n","        'input_dim': embedding_mode_to_input_dim[input_embedding_mode],\n","        'max_input_tokens_length': 300,\n","        'positional_embedding_dim': 200,\n","        'bilstm_hidden_dim': 1024,\n","        'bilstm_layers': 1,\n","        'lstm_embedding_dim': 512,\n","        'projection_embedding_dim': 50,\n","        'temp': 0.1,\n","        'prototype_train_learning_rate': 0.2,\n","        'prototype_train_epochs': 1000,\n","        'prototype_train_patience': 5,\n","        'prototype_num_per_class': 10,\n","        'shot_sample_number': shot_sample_number,\n","        'meta_learning_rate': 0.5,\n","        'task_learning_rate': 5e-4,\n","        'meta_epochs': meta_epochs,\n","        'task_epochs': 3,\n","        'adapt_patience': 100,\n","        'batch_size': 256,\n","        'prototype_loss_weight': 1.0,\n","    }\n","elif input_embedding_mode == 'fasttext':\n","    params = {\n","        'train_test_split_ratio': task_sample_ratio,\n","        'num_classes': len(type_ids),\n","        'dropout': 0.5,\n","        'input_dim': embedding_mode_to_input_dim[input_embedding_mode],\n","        'max_input_tokens_length': 300,\n","        'positional_embedding_dim': 200,\n","        'bilstm_hidden_dim': 1024,\n","        'bilstm_layers': 1,\n","        'lstm_embedding_dim': 512,\n","        'projection_embedding_dim': 50,\n","        'temp': 0.1,\n","        'prototype_train_learning_rate': 0.2,\n","        'prototype_train_epochs': 1000,\n","        'prototype_train_patience': 5,\n","        'prototype_num_per_class': 10,\n","        'shot_sample_number': shot_sample_number,\n","        'meta_learning_rate': 0.4,\n","        'task_learning_rate': 1e-3,\n","        'meta_epochs': meta_epochs,\n","        'task_epochs': 5,\n","        'adapt_patience': 100,\n","        'batch_size': 256,\n","        'prototype_loss_weight': 1.0,\n","    }\n","\n","if ablation == \"multiple_prototype\":\n","    params['prototype_num_per_class'] = 1\n","\n","# Train\n","log_df = pd.DataFrame()\n","\n","# Initialize model\n","model = ReProCon(\n","    params=params,\n","    ffn_hidden_dims=ffn_hidden_dims,\n","    input_embedding_mode=input_embedding_mode,\n","    type_ids=type_ids,\n",")\n","\n","# Initial task set\n","print(\"Making initial task set...\")\n","tasks = make_task_set(task_set_type_id_list_of_samples, params['meta_epochs'], params['shot_sample_number'], len(type_ids))\n","\n","# Initial train\n","add_log_df = train(\n","    model_class=ReProCon,\n","    meta_model=model,\n","    params=params,\n","    ffn_hidden_dims=ffn_hidden_dims,\n","    input_embedding_mode=input_embedding_mode,\n","    type_ids=type_ids,\n","    tasks=tasks,\n","    validation_set_type_id_list_of_samples=validation_set_type_id_list_of_samples,\n","    language_model=language_model,\n","    is_initial=True\n",")\n","log_df = pd.concat([log_df, add_log_df], ignore_index=True, axis=0)\n","\n","if ablation != \"hard_negative\":\n","    # Collect hard negatives\n","    hard_negative_set_type_id_list_of_samples = model.select_hard_negatives(task_set_type_id_list_of_samples, language_model)\n","\n","    # Hard negative task set\n","    print(\"Making hard negative task set...\")\n","    tasks = make_task_set(hard_negative_set_type_id_list_of_samples, params['meta_epochs'], params['shot_sample_number'], len(type_ids))\n","\n","    # Hard negative train\n","    add_log_df = train(\n","        model_class=ReProCon,\n","        meta_model=model,\n","        params=params,\n","        ffn_hidden_dims=ffn_hidden_dims,\n","        input_embedding_mode=input_embedding_mode,\n","        type_ids=type_ids,\n","        tasks=tasks,\n","        validation_set_type_id_list_of_samples=validation_set_type_id_list_of_samples,\n","        language_model=language_model,\n","        is_initial=False\n","    )\n","    log_df = pd.concat([log_df, add_log_df], ignore_index=True, axis=0)\n","\n","# Save log\n","log_df.to_csv(f'{dir}/ablation_study/{ablation}/train_log.csv', index=False)\n","\n","# Save model\n","if input_embedding_mode == 'fasttext':\n","    torch.save(model.bilstm_model.state_dict(), f'{dir}/ablation_study/{ablation}/bilstm_model.pt')\n","torch.save(model.ffn_model.state_dict(), f'{dir}/ablation_study/{ablation}/ffn_model.pt')\n","torch.save(model.prototypes, f'{dir}/ablation_study/{ablation}/prototypes.pt')\n","\n","# Draw heatmap of similarities between prototypes, and then save the heatmap image\n","sim_matrix = torch.mm(model.prototypes, model.prototypes.t())\n","sim_matrix = sim_matrix.cpu().detach().numpy()\n","colors = [\"darkred\", \"white\", \"darkgreen\"]\n","custom_cmap = LinearSegmentedColormap.from_list(\"red_white_green\", colors, N=256)\n","plt.figure(figsize=(14, 12))\n","expanded_type_ids = []\n","for type_id in model.type_ids:\n","    expanded_type_ids.extend([type_id])\n","    expanded_type_ids.extend(['']*(model.prototype_num_per_class-1))\n","sns.heatmap(sim_matrix, annot=False, cmap=custom_cmap, vmin=-1, vmax=1, xticklabels=expanded_type_ids, yticklabels=expanded_type_ids)\n","plt.title('Similarity Matrix between Prototype Vectors')\n","plt.xticks(rotation=45, ha='right')\n","plt.yticks(rotation=0, ha='right')\n","plt.tight_layout()\n","plt.savefig(f'{dir}/ablation_study/{ablation}/prototype_similarity_heatmap.png')\n","\n","# Empty cuda\n","model = model.cpu()\n","del model\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","# Clear others\n","if ablation != \"hard_negative\":\n","    del hard_negative_set_type_id_list_of_samples\n","\n","del tasks, add_log_df, log_df, task_set_type_id_list_of_samples\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIKKLleEOlF0"},"outputs":[],"source":["from google.colab import runtime\n","\n","runtime.unassign()"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}