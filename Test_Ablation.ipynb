{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xxDfO1mZ0hpG"},"outputs":[],"source":["!pip install -q fasttext\n","\n","import math\n","import json\n","import copy\n","import pandas as pd\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score\n","from sklearn.manifold import MDS\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from transformers import AutoTokenizer, BertModel\n","import fasttext\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.colors import LinearSegmentedColormap\n","import gc\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["dir = ''"],"metadata":{"id":"2QxjGOQBMAw3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ablations = [\"hard_negative\", \"multiple_prototype\", \"contrastive_learning\"]\n","ablation = ablations[2]"],"metadata":{"id":"Hj6iBMOVlCge"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEIre3iyvd3Z"},"outputs":[],"source":["# Call device.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load type ids\n","with open(f'{dir}/type_ids.json', 'r') as f:\n","    type_ids = json.load(f)\n","unknown_index = type_ids.index(\"UnknownType\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVishohgvrdR"},"outputs":[],"source":["# Load language model.\n","def load_language_model(input_embedding_mode):\n","    # Initiate language model. The language model will not be fine-tuned to avoid overfitting.\n","    if input_embedding_mode == 'bert':\n","        # Load vanilla bert-base-cased model for input embedding.\n","        tokenizer = AutoTokenizer.from_pretrained(f'{dir}/language_model/{input_embedding_mode}/tokenizer')\n","        pretrained_model = BertModel.from_pretrained(f'{dir}/language_model/{input_embedding_mode}/transformer')\n","        pretrained_model.eval()\n","        pretrained_model.to(device)\n","        pretrained_model.resize_token_embeddings(tokenizer.vocab_size)\n","\n","    elif input_embedding_mode == 'fasttext':\n","        # Load fasttext model for input embedding.\n","        pretrained_model = fasttext.load_model(f'{dir}/language_model/{input_embedding_mode}/cc.en.300.bin')\n","        tokenizer = None\n","\n","    return (tokenizer, pretrained_model)\n","\n","# Define model class.\n","class ReProCon(nn.Module):\n","    def __init__(self, dir, ffn_hidden_dims, input_embedding_mode, params, type_ids):\n","        self.dir=dir\n","        self.ffn_hidden_dims = ffn_hidden_dims\n","        self.input_embedding_mode = input_embedding_mode\n","        self.train_test_split_ratio = params['train_test_split_ratio']\n","        self.shot_sample_number = params['shot_sample_number']\n","        self.max_input_tokens_length = params['max_input_tokens_length']\n","        self.positional_embedding_dim = params['positional_embedding_dim']\n","        self.lstm_embedding_dim = params['lstm_embedding_dim']\n","        self.bilstm_hidden_dim = params['bilstm_hidden_dim']\n","        self.bilstm_layers = params['bilstm_layers']\n","        self.prototype_num_per_class = params['prototype_num_per_class'] ###\n","        self.dropout = params['dropout']\n","        self.input_dim = params['input_dim']\n","        self.batch_size = params['batch_size']\n","        self.temp = params['temp']\n","        self.type_ids = type_ids\n","\n","        super(ReProCon, self).__init__()\n","\n","        # Load prototypes\n","        self.prototypes = torch.load(\n","            f=f'{dir}/ablation_study/{ablation}/prototypes.pt',\n","            map_location=device\n","        )\n","        self.prototypes = self.prototypes.to(device)\n","\n","        # Load BiLSTM model.\n","        self.bilstm_model = nn.LSTM(\n","            input_size=self.input_dim+self.positional_embedding_dim,\n","            hidden_size=self.bilstm_hidden_dim,\n","            num_layers=self.bilstm_layers,\n","            bias=True,\n","            batch_first=True,\n","            dropout=self.dropout,\n","            bidirectional=True,\n","            proj_size=self.lstm_embedding_dim, # Since it is bidirectional, it becomes *2\n","            device=device\n","        )\n","\n","        # Load classification model\n","        if self.input_embedding_mode == 'fasttext':\n","            self.bilstm_model.load_state_dict(\n","                torch.load(\n","                    f=f'{dir}/ablation_study/{ablation}/bilstm_model.pt',\n","                    map_location=device\n","                )\n","            )\n","            self.set_positional_embedding()\n","            self.bilstm_model.eval()\n","            self.bilstm_model.to(device)\n","\n","        ffn_layers = []\n","        ffn_dims = [self.input_dim if self.input_embedding_mode == 'bert' else 2*self.lstm_embedding_dim]\n","        ffn_dims.extend(ffn_hidden_dims)\n","        for i in range(len(ffn_hidden_dims)):\n","            ffn_layers.append(nn.Linear(ffn_dims[i], ffn_dims[i+1]))\n","            ffn_layers.append(nn.BatchNorm1d(ffn_dims[i+1]))\n","            ffn_layers.append(nn.GELU())\n","            ffn_layers.append(nn.Dropout(self.dropout))\n","        ffn_layers.append(nn.Linear(ffn_dims[-1], self.prototypes.shape[1]))\n","        ffn_layers.append(nn.LayerNorm(self.prototypes.shape[1]))\n","        self.ffn_model = nn.Sequential(*ffn_layers)\n","        self.ffn_model.load_state_dict(\n","            torch.load(\n","                f=f'{dir}/ablation_study/{ablation}/ffn_model.pt',\n","                map_location=device\n","            )\n","        )\n","        self.ffn_model.eval()\n","        self.ffn_model.to(device)\n","\n","    def set_positional_embedding(self):\n","        pos = torch.arange(0, self.max_input_tokens_length).unsqueeze(1)\n","        cols = torch.arange(0, self.positional_embedding_dim).unsqueeze(0)\n","        position_tensor = pos / (torch.pow(10000, (2*(cols//2)) / self.positional_embedding_dim))\n","\n","        position_tensor[:, 0::2] = torch.sin(position_tensor[:, 0::2])\n","        position_tensor[:, 1::2] = torch.cos(position_tensor[:, 1::2])\n","\n","        self.positional_embedding = position_tensor.to(device)\n","\n","    def input_embed(self, sample, language_model):\n","        mark_index = sample[0].index(\"[MARK_POSITION]\")\n","        full_sentence = sample[0].copy()\n","        full_sentence[mark_index] = sample[1]\n","\n","        if self.input_embedding_mode == 'bert':\n","            tokenizer, pretrained_model = language_model\n","            named_entity_start_index = 1\n","            named_entity_end_index = 1\n","            for i in range(len(full_sentence)):\n","                if i == mark_index:\n","                    named_entity_encodings = tokenizer.encode(full_sentence[i])\n","                    named_entity_tokens = tokenizer.convert_ids_to_tokens(named_entity_encodings)\n","                    named_entity_tokens.remove(\"[CLS]\")\n","                    named_entity_tokens.remove(\"[SEP]\")\n","                    named_entity_end_index = named_entity_start_index + len(named_entity_tokens)\n","                    break\n","                else:\n","                    not_named_entity_encodings = tokenizer.encode(full_sentence[i])\n","                    not_named_entity_tokens = tokenizer.convert_ids_to_tokens(not_named_entity_encodings)\n","                    not_named_entity_tokens.remove(\"[CLS]\")\n","                    not_named_entity_tokens.remove(\"[SEP]\")\n","                    named_entity_start_index += len(not_named_entity_tokens)\n","\n","            full_sentence_text = ' '.join(full_sentence)\n","            full_sentence_encodings = tokenizer(\n","                full_sentence_text,\n","                max_length=self.max_input_tokens_length,\n","                return_tensors='pt',\n","                padding=\"max_length\",\n","                truncation=True,\n","            )\n","            full_sentence_encodings = {k: v.to(device) for k, v in full_sentence_encodings.items()}\n","            with torch.no_grad():\n","                outputs = pretrained_model(**full_sentence_encodings)\n","                named_entity_embedding = outputs.last_hidden_state[:, named_entity_start_index:named_entity_end_index, :]\n","                if torch.cuda.is_available():\n","                    torch.cuda.synchronize()\n","                max_pool_tensor, _ = torch.max(named_entity_embedding, dim=1)\n","                mean_pool_tensor = torch.mean(named_entity_embedding, dim=1)\n","                span_representation = (max_pool_tensor+mean_pool_tensor)/2\n","\n","                return span_representation, None\n","\n","        elif self.input_embedding_mode == 'fasttext':\n","            _tokenizer, pretrained_model = language_model\n","            forwarded_embedding = torch.zeros(self.max_input_tokens_length, self.input_dim)\n","            for i in range(len(full_sentence)):\n","                tokens = full_sentence[i].split(' ')\n","                if len(tokens) > 1:\n","                    normalized_embeddings = []\n","                    for token in tokens:\n","                        token_vector = pretrained_model.get_word_vector(token)\n","                        norm = np.sqrt(np.sum(token_vector**2))\n","                        if not norm == 0:\n","                            normalized_embeddings.append(token_vector/norm)\n","                        else:\n","                            normalized_embeddings.append(token_vector)\n","\n","                    # Use weighted averaging for better representation\n","                    if len(normalized_embeddings) > 0:\n","                        weights = np.array([1.0 + 0.1*(i-len(normalized_embeddings)/2)**2 for i in range(len(normalized_embeddings))])\n","                        weights = weights / weights.sum()\n","                        mean_vector = np.average(normalized_embeddings, axis=0, weights=weights)\n","                    else:\n","                        mean_vector = np.zeros(self.input_dim)\n","                    span_representation = torch.from_numpy(mean_vector).unsqueeze(0).float().to(device)\n","                    forwarded_embedding[i] = span_representation\n","                else:\n","                    token_vector = pretrained_model.get_word_vector(tokens[0])\n","                    norm = np.sqrt(np.sum(token_vector**2))\n","                    if not norm == 0:\n","                        token_vector = token_vector/norm\n","                    else:\n","                        token_vector = token_vector\n","                    span_representation = torch.from_numpy(token_vector).unsqueeze(0).float().to(device)\n","                    forwarded_embedding[i] = span_representation\n","\n","            forwarded_embedding = forwarded_embedding.to(device)\n","            # This embedding will be forwarded to BiLSTM model. This is because fasttext itself does not reflect position information.\n","            # When embedding via BERT, adding positional embedding and forwarding BiLSTM will be skipped, since BERT model itself already includes this process.\n","            concatenated_embedding = torch.cat((forwarded_embedding, self.positional_embedding), dim=1)\n","\n","            return concatenated_embedding, mark_index\n","\n","    def forward(self, input, mark_index_list):\n","        projection = torch.empty((0, self.prototypes.shape[1]))\n","        projection = projection.to(device)\n","        if self.input_embedding_mode == 'bert':\n","            projection = torch.cat((projection, self.ffn_model.forward(input)), dim=0)\n","\n","        elif self.input_embedding_mode == 'fasttext':\n","            lstm_projection = torch.zeros(len(mark_index_list), self.lstm_embedding_dim*2)\n","            bilstm_forwarded_sequence, _ = self.bilstm_model(input)\n","            for i in range(len(mark_index_list)):\n","                mark_index = mark_index_list[i]\n","                projection_tensor = bilstm_forwarded_sequence[i][mark_index]\n","                lstm_projection[i] = projection_tensor\n","            lstm_projection = lstm_projection.to(device)\n","            ffn_projection = self.ffn_model.forward(lstm_projection)\n","            ffn_projection = ffn_projection.to(device)\n","\n","            projection = torch.cat((projection, ffn_projection), dim=0)\n","\n","        self.prototypes = self.prototypes.to(device)\n","\n","        projection = F.normalize(projection, p=2, dim=1)\n","        prototypes_normalized = self.prototypes\n","        similarity_matrix = torch.mm(projection, prototypes_normalized.t())\n","        square_similarity_matrix = -(1-similarity_matrix)*(1-similarity_matrix)\n","        logits = torch.softmax(square_similarity_matrix/self.temp, dim=1)\n","        classification_result_list = torch.argmax(logits, dim=1).tolist()\n","        for i in range(len(classification_result_list)): ###\n","            classification_result_list[i] = classification_result_list[i] // self.prototype_num_per_class ###\n","\n","        return projection, logits, classification_result_list\n","\n","    # Equivalent to test step in other machine learning process.\n","    def recognize(self, test_set_type_id_list_of_samples, language_model):\n","        self.eval()\n","\n","        y_test_classification_label = []\n","        for key in test_set_type_id_list_of_samples.keys():\n","            y_test_classification_label.extend([int(key)]*len(test_set_type_id_list_of_samples[key]))\n","        y_test_classification_label = torch.tensor(y_test_classification_label)\n","\n","        key_projection_tensor = {}\n","        for i in range(self.prototypes.shape[0]):\n","            empty_tensor = torch.empty((0, self.prototypes.shape[1]))\n","            empty_tensor = empty_tensor.to(torch.device(\"cpu\"))\n","            key_projection_tensor[i] = empty_tensor\n","\n","        # For calculating F1 score.\n","        y_hat_test_classification_label = []\n","\n","        for key in tqdm(test_set_type_id_list_of_samples.keys()):\n","            if self.input_embedding_mode == 'bert':\n","                test_batch_input = torch.zeros(\n","                    self.batch_size,\n","                    self.input_dim\n","                )\n","            elif self.input_embedding_mode == 'fasttext':\n","                test_batch_input = torch.zeros(\n","                    self.batch_size,\n","                    self.max_input_tokens_length,\n","                    self.input_dim+self.positional_embedding_dim\n","                )\n","            test_batch_mark_index_list = []\n","            intra_batch_count = 0\n","            multiple_count = 0\n","            for sample in test_set_type_id_list_of_samples[key]:\n","                input_embedding, mark_index = self.input_embed(sample, language_model)\n","                test_batch_input[intra_batch_count] = input_embedding\n","                if self.input_embedding_mode == 'fasttext':\n","                    test_batch_mark_index_list.append(mark_index)\n","                intra_batch_count += 1\n","                if intra_batch_count == self.batch_size:\n","                    test_batch_input = test_batch_input.to(device)\n","                    test_batch_projection, test_logits, test_batch_classification = self.forward(test_batch_input, test_batch_mark_index_list)\n","                    test_batch_projection_cpu = test_batch_projection.cpu()\n","                    del test_batch_projection\n","                    torch.cuda.empty_cache()\n","                    gc.collect()\n","                    y_hat_test_classification_label.extend(test_batch_classification)\n","                    for i in range(intra_batch_count):\n","                        j_max = 0\n","                        for j in range(self.prototype_num_per_class):\n","                            if test_logits[i][int(key)*self.prototype_num_per_class+j] > test_logits[i][int(key)*self.prototype_num_per_class+j_max]:\n","                                j_max = j\n","                        key_projection_tensor[int(key)*self.prototype_num_per_class+j_max] = torch.cat((key_projection_tensor[int(key)*self.prototype_num_per_class+j_max], test_batch_projection_cpu[i].unsqueeze(0)), dim=0) ###\n","                    multiple_count += 1\n","\n","                    del test_batch_input, test_batch_projection_cpu, test_batch_classification\n","                    gc.collect()\n","\n","                    intra_batch_count = 0\n","                    if self.input_embedding_mode == 'bert':\n","                        test_batch_input = torch.zeros(\n","                            self.batch_size,\n","                            self.input_dim\n","                        )\n","                    elif self.input_embedding_mode == 'fasttext':\n","                        test_batch_input = torch.zeros(\n","                            self.batch_size,\n","                            self.max_input_tokens_length,\n","                            self.input_dim+self.positional_embedding_dim\n","                        )\n","                    test_batch_mark_index_list = []\n","            if intra_batch_count != 0:\n","                test_batch_input = test_batch_input[:intra_batch_count].to(device)\n","                test_batch_projection, test_logits, test_batch_classification = self.forward(test_batch_input, test_batch_mark_index_list)\n","                test_batch_projection_cpu = test_batch_projection.cpu()\n","                del test_batch_projection\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                y_hat_test_classification_label.extend(test_batch_classification)\n","                for i in range(intra_batch_count):\n","                    j_max = 0\n","                    for j in range(self.prototype_num_per_class):\n","                        if test_logits[i][int(key)*self.prototype_num_per_class+j] > test_logits[i][int(key)*self.prototype_num_per_class+j_max]:\n","                            j_max = j\n","                    key_projection_tensor[int(key)*self.prototype_num_per_class+j_max] = torch.cat((key_projection_tensor[int(key)*self.prototype_num_per_class+j_max], test_batch_projection_cpu[i].unsqueeze(0)), dim=0)\n","                del test_batch_input, test_batch_projection_cpu, test_batch_classification\n","                gc.collect()\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","\n","        y_hat_test_classification_label = torch.tensor(y_hat_test_classification_label)\n","\n","        # Calculate F1 score in test set.\n","        total_f1_score_value = f1_score(y_test_classification_label, y_hat_test_classification_label, average='macro')\n","        f1_score_per_type_id = {}\n","        compare_y_test_classification_per_type_id = {}\n","        compare_y_hat_test_classification_per_type_id = {}\n","        for i in range(len(type_ids)):\n","            compare_y_test_classification_per_type_id[type_ids[i]] = []\n","            compare_y_hat_test_classification_per_type_id[type_ids[i]] = []\n","        for i in range(len(y_test_classification_label)):\n","            type_id_for_compare = y_test_classification_label[i]\n","            compare_y_test_classification_per_type_id[type_ids[type_id_for_compare]].append(y_test_classification_label[i])\n","            compare_y_hat_test_classification_per_type_id[type_ids[type_id_for_compare]].append(y_hat_test_classification_label[i])\n","        for type_id in type_ids:\n","            type_id_y_test = torch.tensor(compare_y_test_classification_per_type_id[type_id])\n","            type_id_y_hat_test = torch.tensor(compare_y_hat_test_classification_per_type_id[type_id])\n","            type_id_f1_score = f1_score(type_id_y_test, type_id_y_hat_test, average='macro')\n","            f1_score_per_type_id[type_id] = type_id_f1_score\n","\n","        return total_f1_score_value, f1_score_per_type_id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KaMSFdJTSckI"},"outputs":[],"source":["meta_epochs = 200\n","shot_sample_number = 5\n","input_embedding_mode = 'fasttext'\n","task_sample_ratio = 0.3\n","\n","embedding_mode_to_input_dim = {'bert': 768, 'fasttext': 300}\n","ffn_hidden_dims = [1024]\n","language_model = load_language_model(input_embedding_mode)\n","\n","# Load test_set_type_id_list_of_samples.json\n","with open(f'{dir}/ablation_study/test_set_type_id_list_of_samples.json', 'r') as f:\n","    test_set_type_id_list_of_samples = json.load(f)\n","# I think its enough instead of comparing all projections\n","test_sample_limit = 400\n","for key in test_set_type_id_list_of_samples.keys():\n","    test_set_type_id_list_of_samples[key] = test_set_type_id_list_of_samples[key][:test_sample_limit]\n","\n","if input_embedding_mode == 'bert':\n","    params = {\n","        'train_test_split_ratio': task_sample_ratio,\n","        'num_classes': len(type_ids),\n","        'dropout': 0.5,\n","        'input_dim': embedding_mode_to_input_dim[input_embedding_mode],\n","        'max_input_tokens_length': 300,\n","        'positional_embedding_dim': 200,\n","        'bilstm_hidden_dim': 1024,\n","        'bilstm_layers': 1,\n","        'lstm_embedding_dim': 512,\n","        'projection_embedding_dim': 50,\n","        'temp': 0.1,\n","        'prototype_train_learning_rate': 0.2,\n","        'prototype_train_epochs': 1000,\n","        'prototype_train_patience': 5,\n","        'prototype_num_per_class': 10,\n","        'shot_sample_number': shot_sample_number,\n","        'meta_learning_rate': 0.5,\n","        'task_learning_rate': 5e-4,\n","        'meta_epochs': meta_epochs,\n","        'task_epochs': 3,\n","        'adapt_patience': 100,\n","        'batch_size': 256,\n","        'prototype_loss_weight': 1.0,\n","    }\n","elif input_embedding_mode == 'fasttext':\n","    params = {\n","        'train_test_split_ratio': task_sample_ratio,\n","        'num_classes': len(type_ids),\n","        'dropout': 0.5,\n","        'input_dim': embedding_mode_to_input_dim[input_embedding_mode],\n","        'max_input_tokens_length': 300,\n","        'positional_embedding_dim': 200,\n","        'bilstm_hidden_dim': 1024,\n","        'bilstm_layers': 1,\n","        'lstm_embedding_dim': 512,\n","        'projection_embedding_dim': 50,\n","        'temp': 0.1,\n","        'prototype_train_learning_rate': 0.2,\n","        'prototype_train_epochs': 1000,\n","        'prototype_train_patience': 5,\n","        'prototype_num_per_class': 10,\n","        'shot_sample_number': shot_sample_number,\n","        'meta_learning_rate': 0.4,\n","        'task_learning_rate': 1e-3,\n","        'meta_epochs': meta_epochs,\n","        'task_epochs': 5,\n","        'adapt_patience': 100,\n","        'batch_size': 256,\n","        'prototype_loss_weight': 1.0,\n","    }\n","\n","if ablation == \"multiple_prototype\":\n","    params['prototype_num_per_class'] = 1\n","\n","model = ReProCon(\n","    dir=dir,\n","    ffn_hidden_dims=ffn_hidden_dims,\n","    input_embedding_mode=input_embedding_mode,\n","    params=params,\n","    type_ids=type_ids\n",")\n","\n","total_f1_score_value, f1_score_per_type_id = model.recognize(test_set_type_id_list_of_samples, language_model)\n","\n","results_list = []\n","result = params.copy()\n","result['input_embedding_mode'] = input_embedding_mode\n","result['ffn_hidden_layers'] = len(ffn_hidden_dims)\n","result['ffn_hidden_dims'] = \"-\".join(map(str, ffn_hidden_dims))\n","\n","result['total_f1_score_percent'] = total_f1_score_value*100\n","for type_id in f1_score_per_type_id.keys():\n","    result[f'f1_score_{type_id}_percent'] = f1_score_per_type_id[type_id]*100\n","\n","results_list.append(result)\n","\n","# Save performance results\n","results = pd.DataFrame(results_list)\n","results.to_csv(f'{dir}/ablation_study/{ablation}/performance_result.csv', index=False)\n","\n","# Empty cuda\n","model = model.cpu()\n","del model\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","# Clear others\n","del results, results_list, result, test_set_type_id_list_of_samples\n","gc.collect()"]},{"cell_type":"code","source":["from google.colab import runtime\n","\n","runtime.unassign()"],"metadata":{"id":"sR9o3r5AEhEx"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNOCSPUG65ZNKQaRTowkhow"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}