{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11715,"status":"ok","timestamp":1746617898291,"user":{"displayName":"Luxiant Soltia","userId":"15892199767835272153"},"user_tz":300},"id":"elm9Z6CjC8W-","outputId":"97470057-0d50-4477-a717-23f95a5c9d04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["!pip install -q fasttext\n","\n","import math\n","import json\n","import copy\n","import pandas as pd\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.optim.lr_scheduler import LambdaLR\n","from transformers import AutoTokenizer, BertModel\n","from functools import partial\n","import fasttext\n","import gc\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","dir = '/content/drive/MyDrive/analytics/fewshot_medical'"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"fOPODUxdqSlN","executionInfo":{"status":"ok","timestamp":1746617903031,"user_tz":300,"elapsed":4719,"user":{"displayName":"Luxiant Soltia","userId":"15892199767835272153"}}},"outputs":[],"source":["# Call device.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initiate fixed random seed, for consistent and reproducible output.\n","def set_seed(seed: int = 42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)\n","\n","# Load named_entity_to_type_id, type_ids, generated_text_span.\n","with open(f'{dir}/named_entity_to_type_id.json', 'r') as f:\n","    named_entity_to_type_id = json.load(f)\n","with open(f'{dir}/type_ids.json', 'r') as f:\n","    type_ids = json.load(f)\n","with open(f'{dir}/generated_text_span.json', 'r') as f:\n","    generated_text_span = json.load(f)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"4GsuXnRCxqEL","executionInfo":{"status":"ok","timestamp":1746617903483,"user_tz":300,"elapsed":449,"user":{"displayName":"Luxiant Soltia","userId":"15892199767835272153"}}},"outputs":[],"source":["# Define model class.\n","class EPNet(nn.Module):\n","    def __init__(self, hidden_dims, params, input_embedding_mode):\n","        super().__init__()\n","\n","        # Receive hyperparameters.\n","        self.num_classes = params['num_classes']\n","        self.additional_num_of_unknown_type = params['additional_num_of_unknown_type']\n","        self.dropout = params['dropout']\n","        self.input_dim = params['input_dim']\n","        self.projection_embedding_dim = params['projection_embedding_dim']\n","        self.prototype_train_learning_rate = params['prototype_train_learning_rate']\n","        self.prototype_train_epochs = params['prototype_train_epochs']\n","        self.prototype_train_patience = params['prototype_train_patience']\n","        self.max_span_length = params['max_span_length']\n","        self.length_embedding_dim = params['length_embedding_dim']\n","        self.tau = params['tau']\n","        self.shot_sample_number = params['shot_sample_number']\n","        self.query_sample_number = params['query_sample_number']\n","        self.meta_learning_rate = params['meta_learning_rate']\n","        self.task_ffn_learning_rate = params['task_ffn_learning_rate']\n","        self.meta_epochs = params['meta_epochs']\n","        self.task_epochs = params['task_epochs']\n","        self.adapt_patience = params['adapt_patience']\n","        self.batch_size = params['batch_size']\n","        self.input_embedding_mode = input_embedding_mode\n","\n","        # Added weight for balancing losses\n","        self.span_loss_weight = params.get('span_loss_weight', 1.0)\n","        self.distance_loss_weight = params.get('distance_loss_weight', 0.5)\n","\n","        # Added margin for better class separation\n","        self.margin = params.get('margin', 1.0)\n","\n","        layers = []\n","        dims = [self.input_dim+self.length_embedding_dim]\n","        dims.extend(hidden_dims)\n","\n","        # Hidden layers with batch normalization for better training\n","        for i in range(len(hidden_dims)):\n","            layers.append(nn.Linear(dims[i], dims[i+1]))\n","            layers.append(nn.BatchNorm1d(dims[i+1]))\n","            layers.append(nn.GELU())\n","            layers.append(nn.Dropout(self.dropout))\n","\n","        # Output layer with higher dimensionality for better representation\n","        layers.append(nn.Linear(dims[-1], self.projection_embedding_dim))\n","        layers.append(nn.LayerNorm(self.projection_embedding_dim))\n","\n","        # Project to model.\n","        self.model = nn.Sequential(*layers)\n","\n","        # Prototype vectors\n","        # Additional prototype vectors will be assigned to unkown type, since it would be natural to think that the spans of unknown type will be spreaded diversely on the projection space.\n","        alpha = 20 # Increased for better separation\n","        self.prototypes = nn.Parameter(torch.randn(self.num_classes+self.additional_num_of_unknown_type, self.projection_embedding_dim))\n","        self.prototypes.data = F.normalize(self.prototypes.data, dim=1) * alpha\n","\n","        # Span length embedding with improved initialization\n","        self.length_embeddings = nn.Parameter(torch.randn(self.max_span_length, self.length_embedding_dim))\n","        nn.init.xavier_uniform_(self.length_embeddings)\n","\n","        # Initiate language model. The language model will not be fine-tuned to avoid overfitting.\n","        if input_embedding_mode == 'bert':\n","            # Load vanilla bert-base-cased model for input embedding.\n","            tokenizer = AutoTokenizer.from_pretrained(f'{dir}/language_model/{input_embedding_mode}/tokenizer')\n","            pretrained_model = BertModel.from_pretrained(f'{dir}/language_model/{input_embedding_mode}/transformer')\n","            pretrained_model.eval()\n","            pretrained_model.to(device)\n","            pretrained_model.resize_token_embeddings(tokenizer.vocab_size)\n","            self.tokenizer = tokenizer\n","            self.pretrained_model = pretrained_model\n","        elif input_embedding_mode == 'fasttext':\n","            # Load fasttext model for input embedding.\n","            pretrained_model = fasttext.load_model(f'{dir}/language_model/{input_embedding_mode}/cc.en.300.bin')\n","            self.tokenizer = None\n","            self.pretrained_model = pretrained_model\n","\n","    def input_embed(self, text_span):\n","        length_embedding_tensor = self.length_embeddings[min(len(text_span.split(' '))-1, self.max_span_length-1)].unsqueeze(0)\n","        if self.input_embedding_mode == 'bert':\n","            encodings = self.tokenizer(\n","                text_span,\n","                max_length=128,  # Increased for better context\n","                return_tensors='pt',\n","                padding=\"max_length\",\n","                truncation=True,\n","            )\n","            encodings = encodings.to(device)\n","            with torch.no_grad():\n","                outputs = self.pretrained_model(**encodings)\n","                cls_embedding = outputs.last_hidden_state[:, 0, :]\n","                if torch.cuda.is_available():\n","                    torch.cuda.synchronize()\n","                # Use average pooling alongside max pooling for more robust representation\n","                max_pool_tensor, _ = torch.max(outputs.last_hidden_state, dim=1)\n","                avg_pool_tensor = torch.mean(outputs.last_hidden_state, dim=1)\n","                combined_tensor = (max_pool_tensor + avg_pool_tensor) / 2\n","                if torch.cuda.is_available():\n","                    torch.cuda.synchronize()\n","                span_representation = torch.cat((combined_tensor, length_embedding_tensor), dim=-1)\n","\n","        elif self.input_embedding_mode == 'fasttext':\n","            tokens = text_span.split(' ')\n","            normalized_embeddings = []\n","            for token in tokens:\n","                token_vector = self.pretrained_model.get_word_vector(token)\n","                norm = np.sqrt(np.sum(token_vector**2))\n","                if not norm == 0:\n","                    normalized_embeddings.append(token_vector/norm)\n","                else:\n","                    normalized_embeddings.append(token_vector)\n","            # Use weighted averaging for better representation\n","            if len(normalized_embeddings) > 0:\n","                weights = np.array([1.0 + 0.1 * (i - len(normalized_embeddings)/2)**2 for i in range(len(normalized_embeddings))])\n","                weights = weights / weights.sum()\n","                mean_vector = np.average(normalized_embeddings, axis=0, weights=weights)\n","            else:\n","                mean_vector = np.zeros(self.input_dim)\n","            mean_tensor = torch.from_numpy(mean_vector).unsqueeze(0).float()\n","            span_representation = torch.cat((mean_tensor, length_embedding_tensor), dim=-1)\n","\n","        return span_representation\n","\n","    def forward(self, input):\n","        projection = self.model.forward(input)\n","        # Normalize projections for cosine similarity\n","        projection = F.normalize(projection, p=2, dim=1)\n","        self.prototypes = self.prototypes.to(device)\n","        dists = torch.cdist(projection, self.prototypes, p=2) ** 2\n","        # Apply temperature scaling for sharper probability distribution\n","        temp = 0.1\n","        prediction = F.softmax(-dists/temp, dim=1)\n","        classification_result = torch.argmax(prediction, dim=1)\n","\n","        return projection, prediction, classification_result\n","\n","    # Improved distance loss function with margin\n","    def distance_loss(self):\n","        prototypes = self.prototypes.to(device)\n","        prototypes = F.normalize(prototypes, p=2, dim=1)\n","        num_types = prototypes.size(0)\n","\n","        # Cosine similarity matrix\n","        similarity = torch.mm(prototypes, prototypes.t())\n","\n","        # Remove diagonal elements (self-similarity)\n","        mask = torch.eye(num_types, device=device)\n","        similarity = similarity * (1 - mask)\n","\n","        # Push apart with margin\n","        loss = F.relu(similarity - self.margin).mean()\n","\n","        return loss\n","\n","    # Improved span loss function with focal loss component\n","    def span_loss(self, y_hat_projection, y_classification_label):\n","        y_classification_label_copy = y_classification_label.copy()\n","        y_hat_projection = y_hat_projection.to(device)\n","        y_hat_projection = F.normalize(y_hat_projection, p=2, dim=1)\n","\n","        # Compute distances to prototypes\n","        dist_matrix = torch.cdist(y_hat_projection, self.prototypes, p=2) ** 2\n","\n","        # Assign unknown instances to nearest unknown prototype\n","        for i in range(len(y_classification_label_copy)):\n","            if y_classification_label_copy[i] == 0:\n","                closest = 0\n","                closest_dist = dist_matrix[i][0]\n","                for j in range(self.num_classes, self.num_classes+self.additional_num_of_unknown_type):\n","                    if dist_matrix[i][j] < closest_dist:\n","                        closest = j\n","                        closest_dist = dist_matrix[i][j]\n","                y_classification_label_copy[i] = closest\n","\n","        # Calculate logits with margin\n","        logits = -dist_matrix\n","        y_classification_label_copy = torch.Tensor(y_classification_label_copy).long().to(device)\n","\n","        # Calculate focal loss component - give more weight to hard examples\n","        ce_loss = F.cross_entropy(logits, y_classification_label_copy, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = ((1-pt)**2 * ce_loss).mean()\n","\n","        return focal_loss\n","\n","    # First step with improved initialization\n","    def prototype_train(self):\n","        best_distance_loss = float('inf')\n","        increase_count = 0\n","\n","        self.prototypes.to(device)\n","        optimizer = optim.Adam([self.prototypes], lr=self.prototype_train_learning_rate)\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3)\n","\n","        # Training loop\n","        for epoch in range(self.prototype_train_epochs):\n","            optimizer.zero_grad()\n","            loss = self.distance_loss()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step(loss)\n","\n","            if loss.item() < best_distance_loss:\n","                best_distance_loss = loss.item()\n","                best_prototypes = self.prototypes.detach().clone()\n","                increase_count = 0\n","            else:\n","                increase_count += 1\n","                if increase_count >= self.prototype_train_patience:\n","                    break\n","\n","        self.prototypes.data = best_prototypes.to(device)\n","        # Normalize prototypes after training\n","        self.prototypes.data = F.normalize(self.prototypes.data, p=2, dim=1) * 10\n","        print(f\"Result of initial prototypical network training loss: {best_distance_loss}\")\n","\n","        return\n","\n","    # Modified Davies Bouldin Index (MBDI)\n","    def modified_davies_bouldin_index(self, key_projection_tensor):\n","        prototypes = self.prototypes.to(device)\n","        num_types = self.num_classes\n","        prototype_distances = torch.cdist(prototypes, prototypes, p=2)\n","\n","        key_s_i = {}\n","        for key, projection_tensor in key_projection_tensor.items():\n","            if len(projection_tensor) == 0:\n","                key_s_i[key] = 0.0\n","                continue\n","\n","            projection_tensor = projection_tensor.to(device)\n","            prototype_vector = prototypes[key].unsqueeze(0)\n","            euclidean_distances = torch.cdist(projection_tensor, prototype_vector, p=2)\n","            key_s_i[key] = euclidean_distances.mean().item()  # Using mean instead of std for more stability\n","\n","        key_r_i = {}\n","        for key_i, s_i in key_s_i.items():\n","            if key_i == 0 or key_i >= self.num_classes or s_i == 0.0:\n","                continue\n","\n","            max_r_i = 0\n","            for key_j, s_j in key_s_i.items():\n","                if key_i != key_j and key_j < self.num_classes and s_j > 0.0:\n","                    dist = max(prototype_distances[key_i, key_j].item(), 1e-5)  # Avoid division by zero\n","                    r_i_j = (s_i + s_j) / dist\n","                    if r_i_j > max_r_i:\n","                        max_r_i = r_i_j\n","            key_r_i[key_i] = max_r_i\n","\n","        if len(key_r_i) == 0:\n","            return 0.0\n","        return sum(key_r_i.values()) / len(key_r_i)\n","\n","    # Equivalent to test step in other machine learning process.\n","    def recognize(self, test_set_type_id_list_of_text_span, input_embedding_mode):\n","        self.eval()\n","\n","        # Test set\n","        y_test_classification_label = []\n","        for key in test_set_type_id_list_of_text_span.keys():\n","            for text_span in test_set_type_id_list_of_text_span[key]:\n","                y_test_classification_label.append(key)\n","        y_test_label = torch.tensor(y_test_classification_label)\n","\n","        # For calculating MDBI\n","        key_projection_tensor = {}\n","        for i in range(self.prototypes.shape[0]):\n","            empty_tensor = torch.empty((0, self.projection_embedding_dim))\n","            empty_tensor = empty_tensor.to(device)\n","            key_projection_tensor[i] = empty_tensor\n","\n","        # For calculating F1 score\n","        y_hat_test_classification_label = []\n","        test_batch_input = torch.zeros(self.batch_size, self.input_dim+self.length_embedding_dim)\n","\n","        # Store all predictions for ensemble\n","        all_predictions = []\n","\n","        for key in test_set_type_id_list_of_text_span.keys():\n","            intra_batch_count = 0\n","            for text_span in test_set_type_id_list_of_text_span[key]:\n","                test_batch_input[intra_batch_count] = self.input_embed(text_span)\n","                intra_batch_count += 1\n","                if intra_batch_count == self.batch_size:\n","                    test_batch_input = test_batch_input.to(device)\n","                    test_batch_projection, test_batch_prediction, test_batch_classification = self.forward(test_batch_input)\n","                    for i in range(intra_batch_count):\n","                        key_projection_tensor[key] = torch.cat((key_projection_tensor[key], test_batch_projection[i].unsqueeze(0)), dim=0)\n","                        all_predictions.append(test_batch_classification[i].detach().cpu())\n","                        if test_batch_classification[i].item() >= self.num_classes:\n","                            y_hat_test_classification_label.append(0)\n","                        else:\n","                            y_hat_test_classification_label.append(test_batch_classification[i].item())\n","                    intra_batch_count = 0\n","                    test_batch_input = torch.zeros(self.batch_size, self.input_dim+self.length_embedding_dim)\n","            if intra_batch_count != 0:\n","                test_batch_input = test_batch_input[:intra_batch_count].to(device)\n","                test_batch_projection, test_batch_prediction, test_batch_classification = self.forward(test_batch_input)\n","                for i in range(intra_batch_count):\n","                    key_projection_tensor[key] = torch.cat((key_projection_tensor[key], test_batch_projection[i].unsqueeze(0)), dim=0)\n","                    all_predictions.append(test_batch_classification[i].detach().cpu())\n","                    if test_batch_classification[i].item() >= self.num_classes:\n","                        y_hat_test_classification_label.append(0)\n","                    else:\n","                        y_hat_test_classification_label.append(test_batch_classification[i].item())\n","\n","        y_hat_test_label = torch.tensor(y_hat_test_classification_label)\n","\n","        # Calculate F1 score in test set.\n","        total_f1_score_value = f1_score(y_test_classification_label, y_hat_test_classification_label, average='macro')\n","        f1_score_per_type_id = {}\n","        compare_y_test_classification_per_type_id = {}\n","        compare_y_hat_test_classification_per_type_id = {}\n","        for i in range(len(type_ids)):\n","            compare_y_test_classification_per_type_id[type_ids[i]] = []\n","            compare_y_hat_test_classification_per_type_id[type_ids[i]] = []\n","        for i in range(len(y_test_label)):\n","            type_id_for_compare = y_test_label[i]\n","            compare_y_test_classification_per_type_id[type_ids[type_id_for_compare]].append(y_test_label[i])\n","            compare_y_hat_test_classification_per_type_id[type_ids[type_id_for_compare]].append(y_hat_test_label[i])\n","        for type_id in type_ids:\n","            type_id_y_test = torch.tensor(compare_y_test_classification_per_type_id[type_id])\n","            type_id_y_hat_test = torch.tensor(compare_y_hat_test_classification_per_type_id[type_id])\n","            type_id_f1_score = f1_score(type_id_y_test, type_id_y_hat_test, average='macro')\n","            f1_score_per_type_id[type_id] = type_id_f1_score\n","\n","        # Calculate MDBI.\n","        mdbi = self.modified_davies_bouldin_index(key_projection_tensor)\n","\n","        return total_f1_score_value, f1_score_per_type_id, mdbi\n","\n","    # Model file name formatter\n","    def get_model_filename(self, hidden_dims):\n","        hidden_str = \"-\".join(map(str, hidden_dims))\n","\n","        return f\"model_{hidden_str}_tau_{self.tau}_query_num_{self.query_sample_number}.pt\"\n","\n","# Total training process\n","def proto_meta_train(model_class, params, tasks, input_embedding_mode, hidden_dims):\n","    meta_model = model_class(\n","        hidden_dims=hidden_dims,\n","        params=params,\n","        input_embedding_mode=input_embedding_mode\n","    )\n","    meta_model.to(device)\n","    meta_model.train()\n","\n","    # First step\n","    meta_model.prototype_train()\n","\n","    # Second step\n","\n","    # Assign random tasks.\n","    task_indices = list(range(len(tasks)))\n","    random.shuffle(task_indices)\n","\n","    # Learning rate decay\n","    # Cosine annealing scheduler for better optimization\n","    def cosine_annealing(epoch, total_epochs, eta_min=0):\n","        return eta_min + 0.5 * (model.adapt_learning_rate - eta_min) * (1 + math.cos(math.pi * epoch / total_epochs))\n","\n","    logs = []\n","    patience_counter = 0\n","\n","    # F1 score would be better benchmark.\n","    best_f1_score = 0\n","    best_meta_model_state = None\n","    best_meta_length_embeddings = None\n","    local_maxima_f1_score = 0\n","    local_maxima_meta_model_state = None\n","    local_maxima_meta_length_embeddings = None\n","\n","    for meta_epoch in range(meta_model.meta_epochs):\n","        log = {}\n","        log['meta_epoch'] = meta_epoch+1\n","        log['tau'] = meta_model.tau\n","        log['shot_sample_number'] = meta_model.shot_sample_number\n","        log['query_sample_number'] = meta_model.query_sample_number\n","        log['dropout'] = meta_model.dropout\n","        log['num_classes'] = meta_model.num_classes\n","        log['additional_num_of_unknown_type'] = meta_model.additional_num_of_unknown_type\n","        log['input_dim'] = meta_model.input_dim\n","        log['length_embedding_dim'] = meta_model.length_embedding_dim\n","        log['projection_embedding_dim'] = meta_model.projection_embedding_dim\n","        log['meta_learning_rate'] = meta_model.meta_learning_rate\n","\n","        # Bring task.\n","        (support_type_id_list_of_text_span, validation_type_id_list_of_text_span) = tasks[task_indices[meta_epoch]]\n","\n","        # Clone meta model.\n","        task_model=model_class(\n","            hidden_dims=hidden_dims,\n","            params=params,\n","            input_embedding_mode=input_embedding_mode\n","        )\n","        task_model.model.load_state_dict(meta_model.model.state_dict())\n","        task_model.length_embeddings.data = meta_model.length_embeddings.detach().clone()\n","        task_model.prototypes.data = meta_model.prototypes.detach().clone().to(device)\n","        task_model.to(device)\n","        task_model.train()\n","\n","        # Training FFN is better to be trained separately using separate optimizer, although the code becomes more complicated.\n","        # The previous trial when using unified optimizer, showed that the loss of prototypical network tend to diverge while the loss of FFN tend to converge, meaning that different learning rate should be applied.\n","        # Plus, since the prototypes are already finished training, no further training is required. Rather, it may disturb the optimization of FFN.\n","        # However, when calculating loss for gradient to FFN, distance loss will be included.\n","        task_ffn_optimizer = optim.AdamW(list(task_model.model.parameters()) + [task_model.length_embeddings], lr=meta_model.task_ffn_learning_rate, weight_decay=0.01)\n","        task_ffn_scheduler = optim.lr_scheduler.CosineAnnealingLR(task_ffn_optimizer, T_max=meta_model.task_epochs)\n","\n","        task_old_model_state = task_model.model.state_dict()\n","        task_old_length_embeddings = task_model.length_embeddings.detach().clone()\n","\n","        for task_epoch in range(meta_model.task_epochs):\n","            task_ffn_optimizer.zero_grad()\n","\n","            y_support_classification_label = []\n","            for key in support_type_id_list_of_text_span.keys():\n","                for text_span in support_type_id_list_of_text_span[key]:\n","                    y_support_classification_label.append(key)\n","\n","            y_hat_support_projection = torch.zeros(len(y_support_classification_label), meta_model.projection_embedding_dim)\n","            intra_batch_count = 0\n","            multiple_count = 0\n","            support_batch_input = torch.zeros(meta_model.batch_size, meta_model.input_dim+meta_model.length_embedding_dim)\n","            for key in support_type_id_list_of_text_span.keys():\n","                for text_span in support_type_id_list_of_text_span[key]:\n","                    support_batch_input[intra_batch_count] = meta_model.input_embed(text_span)\n","                    intra_batch_count += 1\n","                    if intra_batch_count == meta_model.batch_size:\n","                        support_batch_input = support_batch_input.to(device)\n","                        support_batch_projection, _, _ = task_model.forward(support_batch_input)\n","                        for i in range(intra_batch_count):\n","                            y_hat_support_projection[multiple_count*meta_model.batch_size+i] = support_batch_projection[i]\n","                        intra_batch_count = 0\n","                        multiple_count += 1\n","                        support_batch_input = torch.zeros(meta_model.batch_size, meta_model.input_dim+meta_model.length_embedding_dim)\n","            if intra_batch_count != 0:\n","                support_batch_input = support_batch_input[:intra_batch_count].to(device)\n","                support_batch_projection, _, _ = task_model.forward(support_batch_input)\n","                for i in range(intra_batch_count):\n","                    y_hat_support_projection[multiple_count*meta_model.batch_size+i] = support_batch_projection[i]\n","\n","            # Calculate support loss with weighted components\n","            supp_loss_d = task_model.distance_loss() * task_model.distance_loss_weight\n","            supp_loss_s = task_model.span_loss(y_hat_support_projection, y_support_classification_label)\n","            supp_loss = supp_loss_d+supp_loss_s\n","            log[f'task_model_epoch_{task_epoch+1}_support_loss'] = supp_loss.item()\n","            log[f'task_model_epoch_{task_epoch+1}_support_loss_d'] = supp_loss_d.item()\n","            log[f'task_model_epoch_{task_epoch+1}_support_loss_s'] = supp_loss_s.item()\n","            print(f\"Task epoch {task_epoch+1}, Support loss {supp_loss:.4f}\")\n","\n","            # Separate backward propagation\n","            log[f'task_model_epoch_{task_epoch+1}_task_ffn_learning_rate'] = task_ffn_optimizer.param_groups[0]['lr']\n","            supp_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(task_model.parameters(), 1.0) # Add gradient clipping\n","            task_ffn_optimizer.step()\n","            task_ffn_scheduler.step()\n","\n","        task_new_model_state = task_model.model.state_dict()\n","        task_new_length_embeddings = task_model.length_embeddings.detach().clone()\n","\n","        with torch.no_grad():\n","            for name, meta_param in meta_model.model.named_parameters():\n","                meta_param.add_(\n","                    (task_new_model_state[name]-task_old_model_state[name])*meta_model.meta_learning_rate\n","                )\n","\n","        updated_length_embeddings = task_old_length_embeddings + (\n","            (task_new_length_embeddings-task_old_length_embeddings)*meta_model.meta_learning_rate\n","        )\n","\n","        meta_model.length_embeddings.data = updated_length_embeddings.to(device)\n","\n","        # Task validation\n","        if task_epoch % 1 == 0: # Evaluate every epoch\n","            meta_model.eval()\n","            with torch.no_grad():\n","                y_validation_classification_label = []\n","                for key in validation_type_id_list_of_text_span.keys():\n","                    for text_span in validation_type_id_list_of_text_span[key]:\n","                        y_validation_classification_label.append(key)\n","\n","                y_hat_validation_classification_label = []\n","                y_hat_validation_projection = torch.zeros(len(y_validation_classification_label), meta_model.projection_embedding_dim)\n","                intra_batch_count = 0\n","                multiple_count = 0\n","                validation_batch_input = torch.zeros(meta_model.batch_size, meta_model.input_dim+meta_model.length_embedding_dim)\n","                for key in validation_type_id_list_of_text_span.keys():\n","                    for text_span in validation_type_id_list_of_text_span[key]:\n","                        validation_batch_input[intra_batch_count] = meta_model.input_embed(text_span)\n","                        intra_batch_count += 1\n","                        if intra_batch_count == meta_model.batch_size:\n","                            validation_batch_input = validation_batch_input.to(device)\n","                            validation_batch_projection, _, validation_batch_classification = meta_model.forward(validation_batch_input)\n","                            for i in range(intra_batch_count):\n","                                y_hat_validation_projection[multiple_count*meta_model.batch_size+i] = validation_batch_projection[i]\n","                                if validation_batch_classification[i].item() >= meta_model.num_classes:\n","                                    y_hat_validation_classification_label.append(0)\n","                                else:\n","                                    y_hat_validation_classification_label.append(validation_batch_classification[i].item())\n","                            intra_batch_count = 0\n","                            multiple_count += 1\n","                            validation_batch_input = torch.zeros(meta_model.batch_size, meta_model.input_dim+meta_model.length_embedding_dim)\n","                if intra_batch_count != 0:\n","                    validation_batch_input = validation_batch_input[:intra_batch_count].to(device)\n","                    validation_batch_projection, _, validation_batch_classification = meta_model.forward(validation_batch_input)\n","                    for i in range(intra_batch_count):\n","                        y_hat_validation_projection[multiple_count*meta_model.batch_size+i] = validation_batch_projection[i]\n","                        if validation_batch_classification[i].item() >= meta_model.num_classes:\n","                            y_hat_validation_classification_label.append(0)\n","                        else:\n","                            y_hat_validation_classification_label.append(validation_batch_classification[i].item())\n","\n","                # Calculate validation F1 score.\n","                val_total_f1_score_value = f1_score(y_validation_classification_label, y_hat_validation_classification_label, average='macro')\n","                log['meta_model_validation_f1_score_percent'] = val_total_f1_score_value*100\n","\n","                # Calculate validation loss.\n","                val_loss_d = meta_model.distance_loss() * meta_model.distance_loss_weight\n","                val_loss_s = meta_model.span_loss(y_hat_validation_projection, y_validation_classification_label) * meta_model.span_loss_weight\n","                val_loss = val_loss_d+val_loss_s\n","\n","                log['meta_model_validation_loss'] = val_loss.item()\n","                log['meta_model_validation_loss_d'] = val_loss_d.item()\n","                log['meta_model_validation_loss_s'] = val_loss_s.item()\n","                print(f\"\\n==== Meta epoch {meta_epoch+1}, F1 score {val_total_f1_score_value*100:.4f}%, Validation loss {val_loss:.4f} ====\\n\")\n","\n","                if val_total_f1_score_value > local_maxima_f1_score:\n","                    local_maxima_f1_score = val_total_f1_score_value\n","                    local_maxima_meta_model_state = meta_model.model.state_dict()\n","                    local_maxima_meta_length_embeddings = meta_model.length_embeddings.detach().clone()\n","                    patience_counter = 0\n","                    if local_maxima_f1_score > best_f1_score:\n","                        best_f1_score = local_maxima_f1_score\n","                        local_maxima_f1_score = 0\n","                        best_meta_model_state = local_maxima_meta_model_state\n","                        best_meta_length_embeddings = local_maxima_meta_length_embeddings.to(device)\n","                else:\n","                    patience_counter += 1\n","                    if patience_counter >= meta_model.adapt_patience:\n","                        break\n","\n","        logs.append(log)\n","\n","    if best_meta_model_state is not None and best_meta_length_embeddings is not None:\n","        meta_model.model.load_state_dict(best_meta_model_state)\n","        meta_model.length_embeddings.data = best_meta_length_embeddings.to(device)\n","    else:\n","        # Fallback to latest model if no improvement found\n","        model.model.load_state_dict(local_maxima_meta_model_state)\n","        model.length_embeddings.data = local_maxima_meta_length_embeddings.to(device)\n","\n","    log_df = pd.DataFrame(logs)\n","    hidden_dims_name = \"-\".join(map(str, hidden_dims))\n","    log_df.to_csv(f'{dir}/{meta_model.shot_sample_number}_shot/{input_embedding_mode}_logs/log_{hidden_dims_name}_tau_{meta_model.tau}_query_num_{meta_model.query_sample_number}.csv', index=False)\n","\n","    return meta_model"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"dMlooEk1yHcC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746617928496,"user_tz":300,"elapsed":25011,"user":{"displayName":"Luxiant Soltia","userId":"15892199767835272153"}},"outputId":"70d838f4-7505-4665-e915-4cdbbb903435"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:04<00:00,  4.17it/s]\n"]}],"source":["# Independent variable:\n","    # shot number per each support set\n","    # types of input embedding\n","# Dependent variable:\n","    # modified davies bouldin index (MDBI)\n","    # F1 score\n","    # F1 score per each classification category.\n","# Control variable:\n","    # dropout (0.5)\n","    # activation function (gelu)\n","    # random seed for layer initiation (42)\n","    # output layer dimension (1024)\n","    # number of hidden layers (1)\n","# Train stop condition: when validation loss increases more than patience number\n","\n","# Embedding mode to input dim map\n","embedding_mode_to_input_dim = {'bert': 768, 'fasttext': 300}\n","\n","# Set hyperparameters.\n","meta_epochs = 1000\n","task_sample_size = 100\n","input_embedding_mode = 'bert' ##\n","shot_sample_number = 1 ##\n","query_sample_number = 20\n","test_sample_number = 15 ## delete this later\n","number_of_hidden_layers = 1\n","\n","# Split test set and task set.\n","total_type_id_list_of_text_span = {}\n","for i in range(len(type_ids)):\n","    total_type_id_list_of_text_span[i] = []\n","for text_span in generated_text_span:\n","    if text_span.strip() == \"\":\n","        continue\n","    type_id = 0\n","    try:\n","        type_id = type_ids.index(named_entity_to_type_id[text_span])\n","    except:\n","        type_id = type_ids.index('UnknownType')\n","    total_type_id_list_of_text_span[type_id].append(text_span)\n","\n","task_set_type_id_list_of_text_span = {}\n","test_set_type_id_list_of_text_span = {}\n","\n","for key in total_type_id_list_of_text_span.keys():\n","    shuffled = total_type_id_list_of_text_span[key]\n","    random.shuffle(shuffled)\n","    task_set_type_id_list_of_text_span[key] = shuffled[:task_sample_size]\n","    test_set_type_id_list_of_text_span[key] = shuffled[task_sample_size:task_sample_size+test_sample_number]\n","\n","# Tasks are combinations of support set and query set.\n","type_id_combinations = {}\n","for type_id, list_of_text_span in tqdm(task_set_type_id_list_of_text_span.items()):\n","    list_of_text_span_copy = list_of_text_span.copy()\n","    combinations = []\n","    while len(combinations) < meta_epochs:\n","        support_pairs = list(tuple(sorted(random.sample(list_of_text_span_copy, shot_sample_number))))\n","        remaining = list(set(list_of_text_span_copy) - set(support_pairs))\n","        query_pairs = list(tuple(sorted(random.sample(remaining, query_sample_number))))\n","        already_in_list = False\n","        for comb in combinations:\n","            if comb[0] == support_pairs and comb[1] == query_pairs:\n","                already_in_list = True\n","                break\n","        if not already_in_list:\n","            combinations.append((support_pairs, query_pairs))\n","\n","    type_id_combinations[type_id] = combinations\n","\n","tasks = []\n","for i in range(meta_epochs):\n","    support_type_id_list_of_text_span = {}\n","    query_type_id_list_of_text_span = {}\n","    for j in range(len(type_ids)):\n","        support_type_id_list_of_text_span[j] = type_id_combinations[j][i][0]\n","        query_type_id_list_of_text_span[j] = type_id_combinations[j][i][1]\n","    tasks.append((support_type_id_list_of_text_span, query_type_id_list_of_text_span))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clfRt5_CyGJo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"743fdab7-ee7d-4da6-a1f1-f4c997f8431a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Result of initial prototypical network training loss: 0.0\n","Task epoch 1, Support loss 3.3843\n","Task epoch 2, Support loss 0.0006\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 1, F1 score 1.5362%, Validation loss 3.2486 ====\n","\n","Task epoch 1, Support loss 3.1534\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 2, F1 score 2.3852%, Validation loss 3.2608 ====\n","\n","Task epoch 1, Support loss 3.2467\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 3, F1 score 2.2397%, Validation loss 3.2492 ====\n","\n","Task epoch 1, Support loss 3.4788\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 4, F1 score 2.3532%, Validation loss 3.2269 ====\n","\n","Task epoch 1, Support loss 3.1755\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 5, F1 score 2.3498%, Validation loss 3.2766 ====\n","\n","Task epoch 1, Support loss 3.2523\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 6, F1 score 2.8158%, Validation loss 3.2423 ====\n","\n","Task epoch 1, Support loss 3.1840\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 7, F1 score 2.3557%, Validation loss 3.2467 ====\n","\n","Task epoch 1, Support loss 3.3047\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 8, F1 score 3.1812%, Validation loss 3.2353 ====\n","\n","Task epoch 1, Support loss 3.5659\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 9, F1 score 1.7518%, Validation loss 3.2621 ====\n","\n","Task epoch 1, Support loss 2.9991\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 10, F1 score 2.0911%, Validation loss 3.2623 ====\n","\n","Task epoch 1, Support loss 3.3123\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 11, F1 score 2.0149%, Validation loss 3.2325 ====\n","\n","Task epoch 1, Support loss 3.2151\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 12, F1 score 2.1096%, Validation loss 3.2712 ====\n","\n","Task epoch 1, Support loss 3.1332\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 13, F1 score 2.1385%, Validation loss 3.2414 ====\n","\n","Task epoch 1, Support loss 3.2401\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 14, F1 score 3.0099%, Validation loss 3.2361 ====\n","\n","Task epoch 1, Support loss 3.3489\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 15, F1 score 2.2246%, Validation loss 3.2440 ====\n","\n","Task epoch 1, Support loss 3.4376\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 16, F1 score 2.0583%, Validation loss 3.2469 ====\n","\n","Task epoch 1, Support loss 3.2750\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 17, F1 score 2.7890%, Validation loss 3.2431 ====\n","\n","Task epoch 1, Support loss 3.0762\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 18, F1 score 2.7305%, Validation loss 3.2318 ====\n","\n","Task epoch 1, Support loss 3.4869\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 19, F1 score 2.3935%, Validation loss 3.2249 ====\n","\n","Task epoch 1, Support loss 3.3229\n","Task epoch 2, Support loss 0.0001\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 20, F1 score 3.5000%, Validation loss 3.2336 ====\n","\n","Task epoch 1, Support loss 3.1525\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 21, F1 score 3.1822%, Validation loss 3.2479 ====\n","\n","Task epoch 1, Support loss 3.3964\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 22, F1 score 2.8091%, Validation loss 3.2698 ====\n","\n","Task epoch 1, Support loss 3.3565\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 23, F1 score 3.2026%, Validation loss 3.2500 ====\n","\n","Task epoch 1, Support loss 3.2267\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 24, F1 score 1.6783%, Validation loss 3.2353 ====\n","\n","Task epoch 1, Support loss 3.3285\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 25, F1 score 2.8002%, Validation loss 3.2311 ====\n","\n","Task epoch 1, Support loss 3.3273\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 26, F1 score 2.3760%, Validation loss 3.2451 ====\n","\n","Task epoch 1, Support loss 3.3080\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 27, F1 score 2.6637%, Validation loss 3.2523 ====\n","\n","Task epoch 1, Support loss 3.6091\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 28, F1 score 3.5493%, Validation loss 3.2202 ====\n","\n","Task epoch 1, Support loss 3.5254\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 29, F1 score 1.8162%, Validation loss 3.2341 ====\n","\n","Task epoch 1, Support loss 3.2676\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 30, F1 score 1.4842%, Validation loss 3.2497 ====\n","\n","Task epoch 1, Support loss 3.4623\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 31, F1 score 2.3113%, Validation loss 3.2411 ====\n","\n","Task epoch 1, Support loss 3.4074\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 32, F1 score 2.9375%, Validation loss 3.2370 ====\n","\n","Task epoch 1, Support loss 3.3037\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 33, F1 score 3.0831%, Validation loss 3.2252 ====\n","\n","Task epoch 1, Support loss 3.3027\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 34, F1 score 2.3722%, Validation loss 3.2658 ====\n","\n","Task epoch 1, Support loss 3.3251\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 35, F1 score 2.2065%, Validation loss 3.2299 ====\n","\n","Task epoch 1, Support loss 3.3176\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 36, F1 score 1.9934%, Validation loss 3.2533 ====\n","\n","Task epoch 1, Support loss 3.2562\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 37, F1 score 2.5234%, Validation loss 3.2375 ====\n","\n","Task epoch 1, Support loss 3.0724\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 38, F1 score 2.1405%, Validation loss 3.2476 ====\n","\n","Task epoch 1, Support loss 3.1790\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 39, F1 score 2.0821%, Validation loss 3.2472 ====\n","\n","Task epoch 1, Support loss 3.4073\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 40, F1 score 3.2391%, Validation loss 3.2519 ====\n","\n","Task epoch 1, Support loss 3.1731\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 41, F1 score 1.8678%, Validation loss 3.2364 ====\n","\n","Task epoch 1, Support loss 3.5680\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 42, F1 score 2.6701%, Validation loss 3.2495 ====\n","\n","Task epoch 1, Support loss 3.2014\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 43, F1 score 2.4080%, Validation loss 3.2565 ====\n","\n","Task epoch 1, Support loss 3.3241\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 44, F1 score 2.6932%, Validation loss 3.2518 ====\n","\n","Task epoch 1, Support loss 3.1993\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 45, F1 score 2.1247%, Validation loss 3.2797 ====\n","\n","Task epoch 1, Support loss 3.1165\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 46, F1 score 2.2136%, Validation loss 3.2435 ====\n","\n","Task epoch 1, Support loss 3.3489\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 47, F1 score 2.0272%, Validation loss 3.2565 ====\n","\n","Task epoch 1, Support loss 3.1804\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 48, F1 score 2.2042%, Validation loss 3.2644 ====\n","\n","Task epoch 1, Support loss 3.2764\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 49, F1 score 3.0624%, Validation loss 3.2366 ====\n","\n","Task epoch 1, Support loss 3.2470\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 50, F1 score 2.7337%, Validation loss 3.2114 ====\n","\n","Task epoch 1, Support loss 3.0674\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 51, F1 score 3.0929%, Validation loss 3.2545 ====\n","\n","Task epoch 1, Support loss 3.5327\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 52, F1 score 2.1396%, Validation loss 3.2483 ====\n","\n","Task epoch 1, Support loss 3.3188\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 53, F1 score 2.4754%, Validation loss 3.2090 ====\n","\n","Task epoch 1, Support loss 3.4652\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 54, F1 score 2.7348%, Validation loss 3.2470 ====\n","\n","Task epoch 1, Support loss 3.1583\n","Task epoch 2, Support loss 0.0001\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 55, F1 score 2.4380%, Validation loss 3.2246 ====\n","\n","Task epoch 1, Support loss 3.3666\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 56, F1 score 1.9027%, Validation loss 3.2516 ====\n","\n","Task epoch 1, Support loss 3.1790\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 57, F1 score 2.2995%, Validation loss 3.2527 ====\n","\n","Task epoch 1, Support loss 3.5237\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 58, F1 score 3.9419%, Validation loss 3.2318 ====\n","\n","Task epoch 1, Support loss 3.5005\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 59, F1 score 2.4739%, Validation loss 3.2808 ====\n","\n","Task epoch 1, Support loss 3.2187\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 60, F1 score 1.9339%, Validation loss 3.2632 ====\n","\n","Task epoch 1, Support loss 3.3974\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 61, F1 score 2.7054%, Validation loss 3.2513 ====\n","\n","Task epoch 1, Support loss 3.3107\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 62, F1 score 2.1632%, Validation loss 3.2436 ====\n","\n","Task epoch 1, Support loss 3.4078\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 63, F1 score 2.1733%, Validation loss 3.2523 ====\n","\n","Task epoch 1, Support loss 3.4500\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 64, F1 score 2.5479%, Validation loss 3.2305 ====\n","\n","Task epoch 1, Support loss 3.1662\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 65, F1 score 2.8611%, Validation loss 3.2462 ====\n","\n","Task epoch 1, Support loss 2.8731\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 66, F1 score 2.5392%, Validation loss 3.2218 ====\n","\n","Task epoch 1, Support loss 3.1722\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 67, F1 score 2.1238%, Validation loss 3.2233 ====\n","\n","Task epoch 1, Support loss 3.6051\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 68, F1 score 1.7046%, Validation loss 3.2481 ====\n","\n","Task epoch 1, Support loss 3.1946\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 69, F1 score 2.5521%, Validation loss 3.2492 ====\n","\n","Task epoch 1, Support loss 3.6621\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 70, F1 score 2.7920%, Validation loss 3.2487 ====\n","\n","Task epoch 1, Support loss 3.3733\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 71, F1 score 2.8312%, Validation loss 3.2537 ====\n","\n","Task epoch 1, Support loss 3.2361\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 72, F1 score 2.7302%, Validation loss 3.2392 ====\n","\n","Task epoch 1, Support loss 3.3982\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 73, F1 score 3.4819%, Validation loss 3.2540 ====\n","\n","Task epoch 1, Support loss 3.4616\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 74, F1 score 1.5603%, Validation loss 3.2841 ====\n","\n","Task epoch 1, Support loss 3.2390\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 75, F1 score 2.0821%, Validation loss 3.2357 ====\n","\n","Task epoch 1, Support loss 3.4005\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 76, F1 score 1.8178%, Validation loss 3.2501 ====\n","\n","Task epoch 1, Support loss 3.2252\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 77, F1 score 2.8060%, Validation loss 3.2475 ====\n","\n","Task epoch 1, Support loss 3.2849\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 78, F1 score 3.2214%, Validation loss 3.2639 ====\n","\n","Task epoch 1, Support loss 3.1604\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 79, F1 score 1.8928%, Validation loss 3.2520 ====\n","\n","Task epoch 1, Support loss 3.0511\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 80, F1 score 2.5919%, Validation loss 3.2620 ====\n","\n","Task epoch 1, Support loss 3.1929\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n","\n","==== Meta epoch 81, F1 score 2.3954%, Validation loss 3.2361 ====\n","\n","Task epoch 1, Support loss 3.2282\n","Task epoch 2, Support loss 0.0000\n","Task epoch 3, Support loss 0.0000\n"]}],"source":["layer_scale = [1024]*number_of_hidden_layers\n","results_list = []\n","\n","tau = 5.0 ##\n","\n","params = {\n","    'dropout': 0.5,\n","    'num_classes': len(type_ids),\n","    'additional_num_of_unknown_type': 10,\n","    'input_dim': embedding_mode_to_input_dim[input_embedding_mode],\n","    'projection_embedding_dim': 1024,\n","    'prototype_train_learning_rate': 0.2,\n","    'prototype_train_epochs': 1000,\n","    'prototype_train_patience': 5,\n","    'max_span_length': 10,\n","    'length_embedding_dim': 25,\n","    'shot_sample_number': shot_sample_number,\n","    'query_sample_number': query_sample_number,\n","    'meta_learning_rate': 0.1,\n","    'task_ffn_learning_rate': 5e-3,\n","    'meta_epochs': meta_epochs,\n","    'task_epochs': 3,\n","    'adapt_patience': 100,\n","    'batch_size': 32,\n","    'tau': tau,\n","}\n","\n","# Train\n","model = proto_meta_train(\n","    model_class=EPNet,\n","    params=params,\n","    tasks=tasks,\n","    input_embedding_mode=input_embedding_mode,\n","    hidden_dims=layer_scale,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLpKm7nbz1W4","executionInfo":{"status":"aborted","timestamp":1746617958682,"user_tz":300,"elapsed":72336,"user":{"displayName":"Luxiant Soltia","userId":"15892199767835272153"}}},"outputs":[],"source":["# Recognize\n","total_f1_score_value, f1_score_per_type_id, mdbi = model.recognize(test_set_type_id_list_of_text_span, input_embedding_mode)\n","print(f\"Total F1 score: {total_f1_score_value*100}%, Modified Davies Bouldin Index: {mdbi}\")\n","print(f\"F1 score per type: {f1_score_per_type_id}\")\n","\n","result = {\n","    'number_of_hidden_layers': number_of_hidden_layers,\n","    'hidden_dims': \"-\".join(map(str, layer_scale)),\n","    'total_f1_score_precent': total_f1_score_value*100,\n","    'mdbi': mdbi\n","}\n","for type_id, f1_score_value in f1_score_per_type_id.items():\n","    result[f'f1_score_{type_id}_percent'] = f1_score_value*100\n","results_list.append(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ujqm52Dz2qD","executionInfo":{"status":"aborted","timestamp":1746617958694,"user_tz":300,"elapsed":2,"user":{"displayName":"Luxiant Soltia","userId":"15892199767835272153"}}},"outputs":[],"source":["# Save model.\n","model_filename = model.get_model_filename(layer_scale)\n","torch.save(model.model.state_dict(), f'{dir}/{model.shot_sample_number}_shot/{input_embedding_mode}_models/model_{model_filename}')\n","torch.save(model.length_embeddings, f'{dir}/{model.shot_sample_number}_shot/{input_embedding_mode}_models/length_embeddings_{model_filename}')\n","torch.save(model.prototypes, f'{dir}/{model.shot_sample_number}_shot/{input_embedding_mode}_models/prototypes_{model_filename}')\n","\n","# Save performance results.\n","results = pd.DataFrame(results_list)\n","results.to_csv(f'{dir}/{model.shot_sample_number}_shot/{input_embedding_mode}_results/performance_result_tau_{tau}.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIKKLleEOlF0","executionInfo":{"status":"aborted","timestamp":1746617958698,"user_tz":300,"elapsed":72347,"user":{"displayName":"Luxiant Soltia","userId":"15892199767835272153"}}},"outputs":[],"source":["from google.colab import runtime\n","\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":0}